{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bb53f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e3e70-ad50-459c-96a7-373b34bcbd5e",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4cb6fecb-bb32-4b35-b3cb-6790a9c63b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y == \"b\")] = -1\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "##############################\n",
    "\n",
    "#standardize the data\n",
    "def standardize(x):\n",
    "\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "##############################\n",
    "\n",
    "#extract each feature of x\n",
    "def extract(lst, nb):\n",
    "    return [item[nb] for item in lst]\n",
    "\n",
    "##############################\n",
    "\n",
    "#create subplot for each feature\n",
    "def subplot(y,x,n):\n",
    "    feat = x[:,n] #extract(x,n)\n",
    "    ax.plot(feat,y,'o')\n",
    "    ax.set_title(f'Feature '+ str(n))\n",
    "    \n",
    "##############################\n",
    "\n",
    "#compute the number of undefined values in a certain feature\n",
    "def nb_undef_feat(x, n):\n",
    "    feat = x[:,n] #extract(x,n)\n",
    "    unique, frequency = np.unique(feat, return_counts = True) #ligne Ã  changer\n",
    "    \n",
    "    if(unique[0] == -999): \n",
    "        return int(frequency[0]) \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "##############################\n",
    "\n",
    "#remove a column from the data\n",
    "def remove_feat(x, n):\n",
    "    d = np.delete(d,n, axis = 1)\n",
    "    return d\n",
    "\n",
    "##############################\n",
    "\n",
    "#remove a feature from a data set that has more than \n",
    "#only put x as argument\n",
    "def remove_feat_w_undef(x):\n",
    "    xdel = []\n",
    "    #xinter= x[0]\n",
    "    for i in range(len(x[0])):\n",
    "        nb = nb_undef_feat(x, i)\n",
    "        if(nb > np.double(0.6*len(x))):\n",
    "            xdel = xdel + [i]\n",
    "        elif(nb > 0):\n",
    "            taille = len(x[:, i])\n",
    "            if(taille % 2 == 0):\n",
    "                index1 = int((taille + nb)/2 - 0.5)\n",
    "                index2 = int((taille + nb)/2 + 0.5)\n",
    "                x_sorted = np.sort(x[:, i])\n",
    "                xmedian = (x_sorted[index1] + x_sorted[index2]) / 2.0\n",
    "            else:\n",
    "                index = int((taille + nb)/2)\n",
    "                xmedian = (np.sort(x[:, i]))[index]\n",
    "\n",
    "            x[:,i][x[:,i] == -999] = xmedian\n",
    " \n",
    "    x = np.delete(x,xdel,axis =1)\n",
    "    return x, xdel\n",
    "       \n",
    "##############################\n",
    "\n",
    "def Multi_Model(tx):\n",
    "#change so that we also keep the indices\n",
    "    tx0 = []\n",
    "    tx1 = []\n",
    "    tx2 = []\n",
    "    tx3 = []\n",
    "    index0 = []\n",
    "    index1 = []\n",
    "    index2 = []\n",
    "    index3 = []\n",
    "    \n",
    "    for i in range(len(tx)):\n",
    "        if(tx[i][22]==0):\n",
    "            tx0 = tx0 + [tx[i]]\n",
    "            index0 = index0 + [i]\n",
    "        else:\n",
    "            if(tx[i][22]==1):\n",
    "                tx1 = tx1 + [tx[i]]\n",
    "                index1 = index1 + [i]\n",
    "            else:\n",
    "                if(tx[i][22]==2):\n",
    "                    tx2 = tx2 + [tx[i]]\n",
    "                    index2 = index2 + [i]\n",
    "                else:\n",
    "                    if(tx[i][22]==3):\n",
    "                        tx3 = tx3 + [tx[i]]\n",
    "                        index3 = index3 + [i]\n",
    "    \n",
    "    return np.array(tx0), np.array(tx1), np.array(tx2), np.array(tx3), index0, index1, index2, index3\n",
    "\n",
    "##############################\n",
    "              \n",
    "#create a csv submission\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})\n",
    "            \n",
    "######################################\n",
    "def build_model_data(height, weight):   \n",
    "    \n",
    "    y = weight\n",
    "    x = height\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx\n",
    "######################################\n",
    "#Cross-validation implementation\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "##########################################\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x),1)) #careful ! \n",
    "    for deg in range(1,degree+1) :\n",
    "        poly = np.c_[poly, np.power(x,deg)]\n",
    "    return poly\n",
    "##########################################\n",
    "#found and replace outliers\n",
    "def id_outliers(x):\n",
    "    for i in range(len(x[0])):\n",
    "        m = np.mean(x[:,i])\n",
    "        std = np.std(x[:,i])\n",
    "        nb = nb_undef_feat(x, i)\n",
    "        \n",
    "        taille = len(x[:, i])\n",
    "        if(taille % 2 == 0):\n",
    "            index1 = int((taille + nb)/2 - 0.5)\n",
    "            index2 = int((taille + nb)/2 + 0.5)\n",
    "            x_sorted = np.sort(x[:, i])\n",
    "            xmedian = (x_sorted[index1] + x_sorted[index2]) / 2.0\n",
    "        else:\n",
    "            index = int((taille + nb)/2)\n",
    "            xmedian = (np.sort(x[:, i]))[index]\n",
    " \n",
    "        x[x[:,i] > m + 3*std] = xmedian\n",
    "        x[x[:,i] < m - 3*std] = xmedian\n",
    "        \n",
    "        return x\n",
    "##########################################################\n",
    "#add more useful features => convert angles in x and y axis [to do after the splitting of classes (+cleaning)?]\n",
    "#to be converted : PRI_tau_phi [15], PRI_lep_phi [18], PRI_met_phi[20], PRI_jet_leading_phi [25] (NDA if 0),PRI_jet_subleading_phi [28] (NDA if 0/1) \n",
    "def to_carthesian(phi):\n",
    "    x = np.cos(phi)\n",
    "    y = np.sin(phi)\n",
    "    \n",
    "    return x,y\n",
    "###########################################################\"\"\n",
    "#This data must be used with the already DIVIDED DATA, but not cleaned\n",
    "def data_to_carthesian(data):\n",
    "    if(data[1][22]==0):\n",
    "        #PRI_tau_phi \n",
    "        colA, colB = to_carthesian(data[:,15])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_lep_phi\n",
    "        colA, colB = to_carthesian(data[:,18])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_met_phi\n",
    "        colA, colB = to_carthesian(data[:,20])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #data.append(to_carthesian(data[:,11]),to_carthesian(data[:,14]),to_carthesian(data[:,16]))\n",
    "        toDelete = [15,18,20]\n",
    "        data = np.delete(data, toDelete, axis=1)\n",
    "        return data\n",
    "        \n",
    "    elif(data[1][22]==1):\n",
    "        #PRI_tau_phi \n",
    "        colA, colB = to_carthesian(data[:,15])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_lep_phi\n",
    "        colA, colB = to_carthesian(data[:,18])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_met_phi\n",
    "        colA, colB = to_carthesian(data[:,20])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_jet_leading_phi\n",
    "        colA, colB = to_carthesian(data[:,25])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #data.append(to_carthesian(data[:,11]),to_carthesian(data[:,14]),to_carthesian(data[:,16]), to_carthesian(data[:,21]))\n",
    "        toDelete = [15,18,20,25]\n",
    "        data = np.delete(data, toDelete, axis=1)\n",
    "        return data\n",
    "    elif(data[1][22]==2 or data[1][22]==3):\n",
    "        #PRI_tau_phi \n",
    "        colA, colB = to_carthesian(data[:,15])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_lep_phi\n",
    "        colA, colB = to_carthesian(data[:,18])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_met_phi\n",
    "        colA, colB = to_carthesian(data[:,20])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_jet_leading_phi\n",
    "        colA, colB = to_carthesian(data[:,25])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_jet_subleading_phi\n",
    "        colA, colB = to_carthesian(data[:,28])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #data.append(to_carthesian(data[:,15]),to_carthesian(data[:,18]),to_carthesian(data[:,20]), to_carthesian(data[:,25]), to_carthesian(data[:,28]))\n",
    "        toDelete = [15,18,20,25,28]\n",
    "        data = np.delete(data, toDelete, axis=1)\n",
    "        return data\n",
    "    else:\n",
    "        print(\"ERROR: there's a mistake in your indexes, darling\")\n",
    "####################################################\n",
    "#create a sublist from a list and a set of index\n",
    "def get_with_index(x, idx):\n",
    "    x_fin = []\n",
    "    \n",
    "    for i in range(len(idx)):\n",
    "        x_fin += [x[idx[i]]]\n",
    "    \n",
    "    return np.array(x_fin)\n",
    "##################################################\n",
    "#recreate a list from 4 sub list and 4 set of index\n",
    "def create_with_index(x0, x1, x2, x3, idx0, idx1, idx2, idx3):\n",
    "    total_length = len(x0) + len(x1) + len(x2) + len(x3)\n",
    "    x_fin = np.repeat(x0[0], total_length).reshape(total_length,)\n",
    "    \n",
    "    for i in range(len(x0)):\n",
    "        x_fin[idx0[i]] = x0[i]\n",
    "    for i in range(len(x1)):\n",
    "        x_fin[idx1[i]] = x1[i]\n",
    "    for i in range(len(x2)):\n",
    "        x_fin[idx2[i]] = x2[i]\n",
    "    for i in range(len(x3)):\n",
    "        x_fin[idx3[i]] = x3[i]\n",
    "        \n",
    "    return x_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86f72593-360d-4401-912a-47b212775529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109715/167336792.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ids = x[:, 0].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_csv_data('train.csv', sub_sample=False) #load the training data\n",
    "test_data = load_csv_data('test.csv', sub_sample=False) #load the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f68fe92f-6d59-4590-a7bb-c80ec3733d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e617e-d43c-414a-ac03-0b3e6488ffdc",
   "metadata": {},
   "source": [
    "## Removing columns with too many undefined the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f699be7d-c88b-43f4-8a87-5b2d1ecb71ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(99913, 20)\n",
      "[[160.937   68.768  103.235  ...   0.725    1.158   46.226 ]\n",
      " [112.4055 162.172  125.953  ...   2.053   -2.028   44.251 ]\n",
      " [154.916   10.418   94.714  ...  -0.715   -1.724   30.638 ]\n",
      " ...\n",
      " [112.4055  78.256   79.699  ...  -0.852   -0.706   78.984 ]\n",
      " [133.457   77.54    88.989  ...  -1.234    2.521   70.969 ]\n",
      " [105.457   60.526   75.839  ...   1.8     -0.166   41.992 ]]\n",
      "[[ 1.38470e+02  5.16550e+01  9.78270e+01 ...  1.24000e+00 -2.47500e+00\n",
      "   1.13497e+02]\n",
      " [ 1.48754e+02  2.88620e+01  1.07782e+02 ...  1.31000e-01 -2.76700e+00\n",
      "   1.79877e+02]\n",
      " [ 1.41481e+02  7.36000e-01  1.11581e+02 ... -7.98000e-01 -2.78500e+00\n",
      "   2.78009e+02]\n",
      " ...\n",
      " [ 1.19934e+02  2.00780e+01  8.87510e+01 ... -1.72500e+00 -2.75600e+00\n",
      "   1.12938e+02]\n",
      " [ 1.26151e+02  2.90230e+01  9.52580e+01 ... -5.99000e-01 -2.52500e+00\n",
      "   1.93099e+02]\n",
      " [ 2.17020e+02  4.71560e+01  6.28240e+01 ... -5.80000e-02 -1.13700e+00\n",
      "   1.74176e+02]]\n",
      "[[ 8.974400e+01  1.355000e+01  5.914900e+01 ...  2.240000e-01\n",
      "   3.106000e+00  1.936600e+02]\n",
      " [ 1.147440e+02  1.028600e+01  7.571200e+01 ...  1.773000e+00\n",
      "  -2.079000e+00  1.656400e+02]\n",
      " [ 1.216810e+02  6.041000e+00  7.320200e+01 ... -1.257000e+00\n",
      "  -6.090000e-01  2.534610e+02]\n",
      " ...\n",
      " [ 1.075345e+02  8.387100e+01  1.708100e+01 ...  3.070000e+00\n",
      "   1.612000e+00  2.718330e+02]\n",
      " [ 1.075345e+02  3.808300e+01  7.499700e+02 ...  5.150000e-01\n",
      "   4.160000e-01  2.035690e+02]\n",
      " [ 1.300750e+02  3.918000e+00  6.678100e+01 ...  5.780000e-01\n",
      "  -2.215000e+00  5.460660e+02]]\n",
      "inside\n",
      "3941\n",
      "277\n",
      "2831\n",
      "0\n",
      "1710\n",
      "3\n",
      "884\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#load data : already done\n",
    "####\n",
    "\n",
    "#separate the data in 4 according to feature index 22\n",
    "zero, un, deux, trois, idx0_train, idx1_train, idx2_train, idx3_train = Multi_Model(train_data[1])\n",
    "\n",
    "#separate the ground true in 4 the data split\n",
    "y0 = get_with_index(y, idx0_train)\n",
    "y1 = get_with_index(y, idx1_train)\n",
    "y2 = get_with_index(y, idx2_train)\n",
    "y3 = get_with_index(y, idx3_train)\n",
    "\n",
    "#zero = data_to_carthesian(zero)\n",
    "#un = data_to_carthesian(un)\n",
    "#deux = data_to_carthesian(deux)\n",
    "#trois = data_to_carthesian(trois)\n",
    "\n",
    "#drop columns in each of the four subdivisions if they have columns with more than 60% of undefined data else it replace the undefined value by the mediane\n",
    "zero_new, xdel0 = remove_feat_w_undef(zero)\n",
    "un_new, xdel1 = remove_feat_w_undef(un)\n",
    "deux_new, xdel2 = remove_feat_w_undef(deux)\n",
    "trois_new, xdel3 = remove_feat_w_undef(trois)\n",
    "\n",
    "#remove the outliers\n",
    "zero_std = id_outliers(zero_new)\n",
    "un_std = id_outliers(un_new)\n",
    "deux_std = id_outliers(deux_new)\n",
    "trois_std = id_outliers(trois_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fbc315c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import implementations\n",
    "\n",
    "#add the first line of 1 in the matrix\n",
    "tx0 = build_poly(zero_std, 1)\n",
    "tx1 = build_poly(un_std, 1)\n",
    "tx2 = build_poly(deux_std, 1)\n",
    "tx3 = build_poly(trois_std, 1)\n",
    "\n",
    "#standardize the cleaned data\n",
    "tx0_std, _, _ = standardize(tx0)\n",
    "\n",
    "tx1_std, _, _ = standardize(tx1)\n",
    "\n",
    "tx2_std, _, _ = standardize(tx2)\n",
    "\n",
    "tx3_std, _, _ = standardize(tx3)\n",
    "\n",
    "#tx0_std = np.delete(tx0_std, [1,3,4,6,11,12,14,15,17,19,20], axis = 1)\n",
    "#tx1_std = np.delete(tx1_std, [1,3,5,6,11,12,13,14,15,16,17,19,21,22], axis = 1)\n",
    "#tx2_std = np.delete(tx2_std, [1,3,8,15,16,17,18,19,21,22,23,25,26,28,29], axis = 1)\n",
    "#tx3_std = np.delete(tx3_std, [1,3,8,9,10,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30], axis = 1)\n",
    "\n",
    "#transform the y from [-1,1] to [0,1]\n",
    "y0_bis = (y0 + 1)/2\n",
    "y1_bis = (y1 + 1)/2\n",
    "y2_bis = (y2 + 1)/2\n",
    "y3_bis = (y3 + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "262519a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the testing set\n",
    "t0, t1, t2, t3, idx0, idx1, idx2, idx3 = Multi_Model(test_data[1])\n",
    "\n",
    "#t0 = data_to_carthesian(t0)\n",
    "#t1 = data_to_carthesian(t1)\n",
    "#t2 = data_to_carthesian(t2)\n",
    "#t3 = data_to_carthesian(t3)\n",
    "\n",
    "#clean the testing set of the non used column\n",
    "t0_cleaned = np.delete(t0, xdel0, axis = 1)\n",
    "t1_cleaned = np.delete(t1, xdel1, axis = 1)\n",
    "t2_cleaned = np.delete(t2, xdel2, axis = 1)\n",
    "t3_cleaned = np.delete(t3, xdel3, axis = 1)\n",
    "\n",
    "#add the first column of 1\n",
    "t0_cleaned = build_poly(t0_cleaned, 1)\n",
    "t1_cleaned = build_poly(t1_cleaned, 1)\n",
    "t2_cleaned = build_poly(t2_cleaned, 1)\n",
    "t3_cleaned = build_poly(t3_cleaned, 1)\n",
    "\n",
    "#standardise the testing data\n",
    "t0_std, _, _ = standardize(t0_cleaned)\n",
    "t1_std, _, _ = standardize(t1_cleaned)\n",
    "t2_std, _, _ = standardize(t2_cleaned)\n",
    "t3_std, _, _ = standardize(t3_cleaned)\n",
    "\n",
    "#t0_std = np.delete(t0_std, [1,3,4,6,11,12,14,15,17,19,20], axis = 1)\n",
    "#t1_std = np.delete(t1_std, [1,3,5,6,11,12,13,14,15,16,17,19,21,22], axis = 1)\n",
    "#t2_std = np.delete(t2_std, [1,3,8,15,16,17,18,19,21,22,23,25,26,28,29], axis = 1)\n",
    "#t3_std = np.delete(t3_std, [1,3,8,9,10,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "8757bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.44301241437380195\n",
      "Current iteration=200, loss=0.43537546219619244\n",
      "Current iteration=300, loss=0.433196472874379\n",
      "Current iteration=400, loss=0.43236328320939366\n",
      "Current iteration=500, loss=0.43195516595189976\n",
      "Current iteration=600, loss=0.43172247683312787\n",
      "Current iteration=700, loss=0.43157436680273126\n",
      "Current iteration=800, loss=0.43147091989897285\n",
      "Current iteration=900, loss=0.4313925960590378\n",
      "Current iteration=1000, loss=0.4313290988674292\n",
      "Current iteration=1100, loss=0.4312746595782273\n",
      "Current iteration=1200, loss=0.43122592856427067\n",
      "Current iteration=1300, loss=0.43118087231990965\n",
      "Current iteration=1400, loss=0.4311382415451685\n",
      "Current iteration=1500, loss=0.4310972583284029\n",
      "Current iteration=1600, loss=0.43105741109883494\n",
      "Current iteration=1700, loss=0.43101838482638233\n",
      "Current iteration=1800, loss=0.43097997854762143\n",
      "Current iteration=1900, loss=0.43094204854516943\n",
      "Current iteration=2000, loss=0.4309045129980513\n",
      "Current iteration=2100, loss=0.43086731233557757\n",
      "Current iteration=2200, loss=0.43083039888457153\n",
      "Current iteration=2300, loss=0.43079375263808034\n",
      "Current iteration=2400, loss=0.43075736161011047\n",
      "Current iteration=2500, loss=0.4307211969844278\n",
      "Current iteration=2600, loss=0.43068525576069777\n",
      "Current iteration=2700, loss=0.43064952985570365\n",
      "Current iteration=2800, loss=0.4306140127632936\n",
      "Current iteration=2900, loss=0.4305786990453543\n",
      "Current iteration=3000, loss=0.4305435839981713\n",
      "Current iteration=3100, loss=0.43050866343305955\n",
      "Current iteration=3200, loss=0.430473933531574\n",
      "Current iteration=3300, loss=0.4304393837771915\n",
      "Current iteration=3400, loss=0.4304050248237487\n",
      "Current iteration=3500, loss=0.43037084648771623\n",
      "Current iteration=3600, loss=0.4303368389004111\n",
      "Current iteration=3700, loss=0.43030301286870115\n",
      "Current iteration=3800, loss=0.4302693519501737\n",
      "Current iteration=3900, loss=0.43023586697037813\n",
      "Current iteration=4000, loss=0.4302025418379426\n",
      "Current iteration=4100, loss=0.43016938736968974\n",
      "Current iteration=4200, loss=0.430136387796431\n",
      "Current iteration=4300, loss=0.43010354737163403\n",
      "Current iteration=4400, loss=0.4300708702465887\n",
      "Current iteration=4500, loss=0.43003834110316536\n",
      "Current iteration=4600, loss=0.43000596425973664\n",
      "Current iteration=4700, loss=0.42997373755377244\n",
      "Current iteration=4800, loss=0.42994165887911856\n",
      "Current iteration=4900, loss=0.42990972618391426\n",
      "Current iteration=5000, loss=0.42987793746863373\n",
      "Current iteration=5100, loss=0.42984629078422915\n",
      "Current iteration=5200, loss=0.42981478423036623\n",
      "Current iteration=5300, loss=0.42978341595374064\n",
      "Current iteration=5400, loss=0.42975217804546106\n",
      "Current iteration=5500, loss=0.42972108098051753\n",
      "Current iteration=5600, loss=0.42969011087500675\n",
      "Current iteration=5700, loss=0.4296592781321899\n",
      "Current iteration=5800, loss=0.4296285691020588\n",
      "Current iteration=5900, loss=0.42959799411460964\n",
      "Current iteration=6000, loss=0.42956753974471584\n",
      "Current iteration=6100, loss=0.42953721624785784\n",
      "Current iteration=6200, loss=0.4295070104153252\n",
      "Current iteration=6300, loss=0.42947692666003773\n",
      "Current iteration=6400, loss=0.4294469635521546\n",
      "Current iteration=6500, loss=0.429417119693432\n",
      "Current iteration=6600, loss=0.42938739371624124\n",
      "Current iteration=6700, loss=0.4293577842826262\n",
      "Current iteration=6800, loss=0.4293282900833962\n",
      "Current iteration=6900, loss=0.42929890983725266\n",
      "Current iteration=7000, loss=0.42926963677095037\n",
      "Current iteration=7100, loss=0.4292404807277663\n",
      "Current iteration=7200, loss=0.4292114349524972\n",
      "Current iteration=7300, loss=0.4291824928499085\n",
      "Current iteration=7400, loss=0.42915366413313055\n",
      "Current iteration=7500, loss=0.42912493687097625\n",
      "Current iteration=7600, loss=0.4290963153718106\n",
      "Current iteration=7700, loss=0.429067803831626\n",
      "Current iteration=7800, loss=0.4290393905854654\n",
      "Current iteration=7900, loss=0.4290110799052024\n",
      "Current iteration=8000, loss=0.42898287076623126\n",
      "Current iteration=8100, loss=0.4289547621636208\n",
      "Current iteration=8200, loss=0.4289267531115794\n",
      "Current iteration=8300, loss=0.4288988426429361\n",
      "Current iteration=8400, loss=0.42887102980864356\n",
      "Current iteration=8500, loss=0.4288433086490568\n",
      "Current iteration=8600, loss=0.42881568833655304\n",
      "Current iteration=8700, loss=0.4287881629149006\n",
      "Current iteration=8800, loss=0.4287607265664193\n",
      "Current iteration=8900, loss=0.42873338832778135\n",
      "Current iteration=9000, loss=0.42870613750691766\n",
      "Current iteration=9100, loss=0.4286789782044414\n",
      "Current iteration=9200, loss=0.42865190959921645\n",
      "Current iteration=9300, loss=0.42862493566988896\n",
      "Current iteration=9400, loss=0.4285980460232892\n",
      "Current iteration=9500, loss=0.4285712446956594\n",
      "Current iteration=9600, loss=0.42854453092122363\n",
      "Current iteration=9700, loss=0.42851789927788914\n",
      "Current iteration=9800, loss=0.42849135839240127\n",
      "Current iteration=9900, loss=0.42846490283928523\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5670099117687184\n",
      "Current iteration=200, loss=0.5817352365038239\n",
      "Current iteration=300, loss=0.5689358236575788\n",
      "Current iteration=400, loss=0.5629993602127848\n",
      "Current iteration=500, loss=0.5591448713704382\n",
      "Current iteration=600, loss=0.5565384510184798\n",
      "Current iteration=700, loss=0.5548610926077938\n",
      "Current iteration=800, loss=0.5539267698242659\n",
      "Current iteration=900, loss=0.5534896555130036\n",
      "Current iteration=1000, loss=0.5532756306155902\n",
      "Current iteration=1100, loss=0.5531365702967672\n",
      "Current iteration=1200, loss=0.5530296377175598\n",
      "Current iteration=1300, loss=0.5529427062537906\n",
      "Current iteration=1400, loss=0.5528696701318163\n",
      "Current iteration=1500, loss=0.5528065089874817\n",
      "Current iteration=1600, loss=0.5527504572987315\n",
      "Current iteration=1700, loss=0.552699586510276\n",
      "Current iteration=1800, loss=0.5526525367400319\n",
      "Current iteration=1900, loss=0.5526083391784363\n",
      "Current iteration=2000, loss=0.5525662969705347\n",
      "Current iteration=2100, loss=0.5525259043802448\n",
      "Current iteration=2200, loss=0.5524867913195164\n",
      "Current iteration=2300, loss=0.5524486848772897\n",
      "Current iteration=2400, loss=0.5524113823673034\n",
      "Current iteration=2500, loss=0.5523747322633832\n",
      "Current iteration=2600, loss=0.5523386205905194\n",
      "Current iteration=2700, loss=0.5523029611265694\n",
      "Current iteration=2800, loss=0.5522676882904491\n",
      "Current iteration=2900, loss=0.552232751941256\n",
      "Current iteration=3000, loss=0.5521981135482372\n",
      "Current iteration=3100, loss=0.5521637433520948\n",
      "Current iteration=3200, loss=0.5521296182486066\n",
      "Current iteration=3300, loss=0.552095720202284\n",
      "Current iteration=3400, loss=0.5520620350515226\n",
      "Current iteration=3500, loss=0.5520285516046717\n",
      "Current iteration=3600, loss=0.5519952609534878\n",
      "Current iteration=3700, loss=0.5519621559498497\n",
      "Current iteration=3800, loss=0.5519292308056489\n",
      "Current iteration=3900, loss=0.5518964807860053\n",
      "Current iteration=4000, loss=0.5518639019734428\n",
      "Current iteration=4100, loss=0.5518314910862172\n",
      "Current iteration=4200, loss=0.5517992453380756\n",
      "Current iteration=4300, loss=0.5517671623298156\n",
      "Current iteration=4400, loss=0.5517352399652942\n",
      "Current iteration=4500, loss=0.5517034763862889\n",
      "Current iteration=4600, loss=0.5516718699219085\n",
      "Current iteration=4700, loss=0.5516404190492635\n",
      "Current iteration=4800, loss=0.5516091223628539\n",
      "Current iteration=4900, loss=0.5515779785507183\n",
      "Current iteration=5000, loss=0.5515469863758347\n",
      "Current iteration=5100, loss=0.5515161446616004\n",
      "Current iteration=5200, loss=0.5514854522804868\n",
      "Current iteration=5300, loss=0.5514549081451658\n",
      "Current iteration=5400, loss=0.5514245112015604\n",
      "Current iteration=5500, loss=0.5513942604233972\n",
      "Current iteration=5600, loss=0.5513641548079287\n",
      "Current iteration=5700, loss=0.5513341933725704\n",
      "Current iteration=5800, loss=0.5513043751522496\n",
      "Current iteration=5900, loss=0.5512746991973141\n",
      "Current iteration=6000, loss=0.5512451645718734\n",
      "Current iteration=6100, loss=0.5512157703524824\n",
      "Current iteration=6200, loss=0.5511865156270921\n",
      "Current iteration=6300, loss=0.5511573994942068\n",
      "Current iteration=6400, loss=0.5511284210622055\n",
      "Current iteration=6500, loss=0.5510995794487936\n",
      "Current iteration=6600, loss=0.55107087378055\n",
      "Current iteration=6700, loss=0.551042303192558\n",
      "Current iteration=6800, loss=0.5510138668280956\n",
      "Current iteration=6900, loss=0.5509855638383747\n",
      "Current iteration=7000, loss=0.5509573933823192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=7100, loss=0.5509293546263737\n",
      "Current iteration=7200, loss=0.5509014467443347\n",
      "Current iteration=7300, loss=0.5508736689172031\n",
      "Current iteration=7400, loss=0.5508460203330506\n",
      "Current iteration=7500, loss=0.5508185001868973\n",
      "Current iteration=7600, loss=0.5507911076806012\n",
      "Current iteration=7700, loss=0.5507638420227551\n",
      "Current iteration=7800, loss=0.5507367024285877\n",
      "Current iteration=7900, loss=0.5507096881198738\n",
      "Current iteration=8000, loss=0.550682798324845\n",
      "Current iteration=8100, loss=0.550656032278107\n",
      "Current iteration=8200, loss=0.5506293892205588\n",
      "Current iteration=8300, loss=0.5506028683993144\n",
      "Current iteration=8400, loss=0.5505764690676272\n",
      "Current iteration=8500, loss=0.5505501904848167\n",
      "Current iteration=8600, loss=0.5505240319161965\n",
      "Current iteration=8700, loss=0.5504979926330037\n",
      "Current iteration=8800, loss=0.5504720719123309\n",
      "Current iteration=8900, loss=0.5504462690370592\n",
      "Current iteration=9000, loss=0.5504205832957908\n",
      "Current iteration=9100, loss=0.5503950139827859\n",
      "Current iteration=9200, loss=0.5503695603978979\n",
      "Current iteration=9300, loss=0.5503442218465114\n",
      "Current iteration=9400, loss=0.5503189976394808\n",
      "Current iteration=9500, loss=0.5502938870930694\n",
      "Current iteration=9600, loss=0.5502688895288904\n",
      "Current iteration=9700, loss=0.5502440042738477\n",
      "Current iteration=9800, loss=0.5502192306600794\n",
      "Current iteration=9900, loss=0.5501945680248996\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5482841063893213\n",
      "Current iteration=200, loss=0.5478561210390276\n",
      "Current iteration=300, loss=0.5422125826200579\n",
      "Current iteration=400, loss=0.535129981148206\n",
      "Current iteration=500, loss=0.5311514278692333\n",
      "Current iteration=600, loss=0.5285746927247822\n",
      "Current iteration=700, loss=0.5268024350509357\n",
      "Current iteration=800, loss=0.5255919607736662\n",
      "Current iteration=900, loss=0.5247828551597409\n",
      "Current iteration=1000, loss=0.5242094441015236\n",
      "Current iteration=1100, loss=0.52375713998296\n",
      "Current iteration=1200, loss=0.5233805456272012\n",
      "Current iteration=1300, loss=0.5230621891603001\n",
      "Current iteration=1400, loss=0.5227912380245573\n",
      "Current iteration=1500, loss=0.522559370743758\n",
      "Current iteration=1600, loss=0.5223599703538918\n",
      "Current iteration=1700, loss=0.5221877165576675\n",
      "Current iteration=1800, loss=0.5220382932611153\n",
      "Current iteration=1900, loss=0.5219081707404444\n",
      "Current iteration=2000, loss=0.5217944412756353\n",
      "Current iteration=2100, loss=0.5216946937091186\n",
      "Current iteration=2200, loss=0.521606916687356\n",
      "Current iteration=2300, loss=0.5215294232799316\n",
      "Current iteration=2400, loss=0.5214607917041775\n",
      "Current iteration=2500, loss=0.5213998183081745\n",
      "Current iteration=2600, loss=0.5213454799746533\n",
      "Current iteration=2700, loss=0.5212969038316241\n",
      "Current iteration=2800, loss=0.5212533426790806\n",
      "Current iteration=2900, loss=0.5212141549238689\n",
      "Current iteration=3000, loss=0.5211787880972863\n",
      "Current iteration=3100, loss=0.5211467652404279\n",
      "Current iteration=3200, loss=0.5211176736004252\n",
      "Current iteration=3300, loss=0.5210911552006006\n",
      "Current iteration=3400, loss=0.521066898939141\n",
      "Current iteration=3500, loss=0.5210446339414296\n",
      "Current iteration=3600, loss=0.5210241239458896\n",
      "Current iteration=3700, loss=0.5210051625459461\n",
      "Current iteration=3800, loss=0.5209875691443507\n",
      "Current iteration=3900, loss=0.5209711855027284\n",
      "Current iteration=4000, loss=0.5209558727904233\n",
      "Current iteration=4100, loss=0.520941509053714\n",
      "Current iteration=4200, loss=0.5209279870401526\n",
      "Current iteration=4300, loss=0.52091521232388\n",
      "Current iteration=4400, loss=0.5209031016867836\n",
      "Current iteration=4500, loss=0.5208915817177523\n",
      "Current iteration=4600, loss=0.5208805875983403\n",
      "Current iteration=4700, loss=0.5208700620481513\n",
      "Current iteration=4800, loss=0.5208599544073997\n",
      "Current iteration=4900, loss=0.5208502198375398\n",
      "Current iteration=5000, loss=0.520840818623733\n",
      "Current iteration=5100, loss=0.5208317155653238\n",
      "Current iteration=5200, loss=0.5208228794425124\n",
      "Current iteration=5300, loss=0.5208142825491117\n",
      "Current iteration=5400, loss=0.5208059002827201\n",
      "Current iteration=5500, loss=0.5207977107848439\n",
      "Current iteration=5600, loss=0.5207896946245534\n",
      "Current iteration=5700, loss=0.5207818345201186\n",
      "Current iteration=5800, loss=0.5207741150938391\n",
      "Current iteration=5900, loss=0.5207665226559054\n",
      "Current iteration=6000, loss=0.5207590450137005\n",
      "Current iteration=6100, loss=0.5207516713033983\n",
      "Current iteration=6200, loss=0.5207443918411476\n",
      "Current iteration=6300, loss=0.5207371979914597\n",
      "Current iteration=6400, loss=0.5207300820507318\n",
      "Current iteration=6500, loss=0.5207230371440971\n",
      "Current iteration=6600, loss=0.5207160571340201\n",
      "Current iteration=6700, loss=0.520709136539247\n",
      "Current iteration=6800, loss=0.5207022704629011\n",
      "Current iteration=6900, loss=0.5206954545286562\n",
      "Current iteration=7000, loss=0.5206886848240471\n",
      "Current iteration=7100, loss=0.5206819578500989\n",
      "Current iteration=7200, loss=0.5206752704765483\n",
      "Current iteration=7300, loss=0.5206686199020164\n",
      "Current iteration=7400, loss=0.5206620036185731\n",
      "Current iteration=7500, loss=0.5206554193801934\n",
      "Current iteration=7600, loss=0.5206488651746708\n",
      "Current iteration=7700, loss=0.5206423391985947\n",
      "Current iteration=7800, loss=0.5206358398350555\n",
      "Current iteration=7900, loss=0.5206293656337675\n",
      "Current iteration=8000, loss=0.5206229152933474\n",
      "Current iteration=8100, loss=0.5206164876455045\n",
      "Current iteration=8200, loss=0.5206100816409376\n",
      "Current iteration=8300, loss=0.5206036963367451\n",
      "Current iteration=8400, loss=0.5205973308851887\n",
      "Current iteration=8500, loss=0.5205909845236573\n",
      "Current iteration=8600, loss=0.5205846565657067\n",
      "Current iteration=8700, loss=0.5205783463930509\n",
      "Current iteration=8800, loss=0.5205720534484088\n",
      "Current iteration=8900, loss=0.5205657772291096\n",
      "Current iteration=9000, loss=0.5205595172813767\n",
      "Current iteration=9100, loss=0.5205532731952176\n",
      "Current iteration=9200, loss=0.5205470445998531\n",
      "Current iteration=9300, loss=0.5205408311596301\n",
      "Current iteration=9400, loss=0.5205346325703646\n",
      "Current iteration=9500, loss=0.5205284485560701\n",
      "Current iteration=9600, loss=0.5205222788660301\n",
      "Current iteration=9700, loss=0.5205161232721782\n",
      "Current iteration=9800, loss=0.520509981566752\n",
      "Current iteration=9900, loss=0.520503853560194\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5630896857619592\n",
      "Current iteration=200, loss=0.5482534530182982\n",
      "Current iteration=300, loss=0.5436621641444765\n",
      "Current iteration=400, loss=0.5413984698844264\n",
      "Current iteration=500, loss=0.5407833232073782\n",
      "Current iteration=600, loss=0.539268020428114\n",
      "Current iteration=700, loss=0.5377636098567657\n",
      "Current iteration=800, loss=0.5367847184542871\n",
      "Current iteration=900, loss=0.5361164186921435\n",
      "Current iteration=1000, loss=0.5356013122022738\n",
      "Current iteration=1100, loss=0.5351734981008163\n",
      "Current iteration=1200, loss=0.5348078982696499\n",
      "Current iteration=1300, loss=0.5344917532857038\n",
      "Current iteration=1400, loss=0.5342163046858059\n",
      "Current iteration=1500, loss=0.5339748331014001\n",
      "Current iteration=1600, loss=0.5337620089071716\n",
      "Current iteration=1700, loss=0.5335735355008261\n",
      "Current iteration=1800, loss=0.5334059073268624\n",
      "Current iteration=1900, loss=0.5332562355371067\n",
      "Current iteration=2000, loss=0.533122118824547\n",
      "Current iteration=2100, loss=0.5330015458032087\n",
      "Current iteration=2200, loss=0.5328928200478267\n",
      "Current iteration=2300, loss=0.5327945017729182\n",
      "Current iteration=2400, loss=0.5327053619507693\n",
      "Current iteration=2500, loss=0.532624345866107\n",
      "Current iteration=2600, loss=0.5325505439186964\n",
      "Current iteration=2700, loss=0.5324831680518288\n",
      "Current iteration=2800, loss=0.5324215325881181\n",
      "Current iteration=2900, loss=0.5323650385464636\n",
      "Current iteration=3000, loss=0.5323131607292361\n",
      "Current iteration=3100, loss=0.5322654370291053\n",
      "Current iteration=3200, loss=0.5322214595257514\n",
      "Current iteration=3300, loss=0.5321808670345853\n",
      "Current iteration=3400, loss=0.5321433388400824\n",
      "Current iteration=3500, loss=0.5321085894007843\n",
      "Current iteration=3600, loss=0.532076363855412\n",
      "Current iteration=3700, loss=0.5320464341927083\n",
      "Current iteration=3800, loss=0.5320185959737791\n",
      "Current iteration=3900, loss=0.531992665516422\n",
      "Current iteration=4000, loss=0.5319684774674228\n",
      "Current iteration=4100, loss=0.5319458827020207\n",
      "Current iteration=4200, loss=0.5319247465003696\n",
      "Current iteration=4300, loss=0.531904946959418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4400, loss=0.5318863736056086\n",
      "Current iteration=4500, loss=0.5318689261794892\n",
      "Current iteration=4600, loss=0.5318525135679907\n",
      "Current iteration=4700, loss=0.5318370528639615\n",
      "Current iteration=4800, loss=0.5318224685357154\n",
      "Current iteration=4900, loss=0.5318086916919663\n",
      "Current iteration=5000, loss=0.5317956594297218\n",
      "Current iteration=5100, loss=0.5317833142545099\n",
      "Current iteration=5200, loss=0.5317716035638589\n",
      "Current iteration=5300, loss=0.5317604791862203\n",
      "Current iteration=5400, loss=0.5317498969686162\n",
      "Current iteration=5500, loss=0.5317398164072031\n",
      "Current iteration=5600, loss=0.5317302003157236\n",
      "Current iteration=5700, loss=0.5317210145274808\n",
      "Current iteration=5800, loss=0.531712227627029\n",
      "Current iteration=5900, loss=0.5317038107082688\n",
      "Current iteration=6000, loss=0.5316957371560375\n",
      "Current iteration=6100, loss=0.5316879824486519\n",
      "Current iteration=6200, loss=0.5316805239791662\n",
      "Current iteration=6300, loss=0.5316733408933748\n",
      "Current iteration=6400, loss=0.5316664139428192\n",
      "Current iteration=6500, loss=0.5316597253512639\n",
      "Current iteration=6600, loss=0.5316532586932776\n",
      "Current iteration=6700, loss=0.5316469987837112\n",
      "Current iteration=6800, loss=0.5316409315769951\n",
      "Current iteration=6900, loss=0.5316350440753034\n",
      "Current iteration=7000, loss=0.5316293242447234\n",
      "Current iteration=7100, loss=0.5316237609386738\n",
      "Current iteration=7200, loss=0.5316183438278833\n",
      "Current iteration=7300, loss=0.531613063336321\n",
      "Current iteration=7400, loss=0.5316079105825255\n",
      "Current iteration=7500, loss=0.5316028773258422\n",
      "Current iteration=7600, loss=0.5315979559171221\n",
      "Current iteration=7700, loss=0.5315931392534815\n",
      "Current iteration=7800, loss=0.5315884207367638\n",
      "Current iteration=7900, loss=0.5315837942353744\n",
      "Current iteration=8000, loss=0.5315792540491941\n",
      "Current iteration=8100, loss=0.5315747948773075\n",
      "Current iteration=8200, loss=0.5315704117882981\n",
      "Current iteration=8300, loss=0.5315661001928972\n",
      "Current iteration=8400, loss=0.5315618558187832\n",
      "Current iteration=8500, loss=0.5315576746873526\n",
      "Current iteration=8600, loss=0.5315535530922959\n",
      "Current iteration=8700, loss=0.531549487579832\n",
      "Current iteration=8800, loss=0.5315454749304613\n",
      "Current iteration=8900, loss=0.5315415121421149\n",
      "Current iteration=9000, loss=0.5315375964145855\n",
      "Current iteration=9100, loss=0.5315337251351384\n",
      "Current iteration=9200, loss=0.5315298958652043\n",
      "Current iteration=9300, loss=0.5315261063280713\n",
      "Current iteration=9400, loss=0.5315223543974941\n",
      "Current iteration=9500, loss=0.5315186380871492\n",
      "Current iteration=9600, loss=0.5315149555408694\n",
      "Current iteration=9700, loss=0.5315113050235967\n",
      "Current iteration=9800, loss=0.531507684912998\n",
      "Current iteration=9900, loss=0.5315040936916917\n",
      "(227458,)\n",
      "227458\n",
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "#start the training of the 4 logistic regression (each on a different subset)\n",
    "loss0, w0 = implementations.reg_logistic_regression(y0_bis, tx0_std, 0.0000035, np.ones(len(tx0_std[0])), 10000, 0.852)\n",
    "loss1, w1 = implementations.reg_logistic_regression(y1_bis, tx1_std, 0.0000035, np.ones(len(tx1_std[0])), 10000, 0.532)\n",
    "loss2, w2 = implementations.reg_logistic_regression(y2_bis, tx2_std, 0.0000035, np.ones(len(tx2_std[0])), 10000, 0.701)\n",
    "loss3, w3 = implementations.reg_logistic_regression(y3_bis, tx3_std, 0.0000035, np.ones(len(tx3_std[0])), 10000, 0.456)\n",
    "\n",
    "\"\"\"\n",
    "loss0, w0 = implementations.mean_squared_error_gd(y0_bis, tx0, np.ones(len(tx0[0])), 10000, 0.05)\n",
    "loss1, w1 = implementations.mean_squared_error_gd(y1_bis, tx1, np.ones(len(tx1[0])), 10000, 0.05)\n",
    "loss2, w2 = implementations.mean_squared_error_gd(y2_bis, tx2, np.ones(len(tx2[0])), 10000, 0.05)\n",
    "loss3, w3 = implementations.mean_squared_error_gd(y3_bis, tx3, np.ones(len(tx3[0])), 10000, 0.05)\n",
    "\"\"\"\n",
    "\n",
    "#Use the weight found by the above training to predict the value of the testing set\n",
    "r0 = implementations.sigmoid(t0_std @ w0)\n",
    "r0_pred = np.sign(r0 - 0.5) #the sign function is used to inverse the y transformation and go from [0, 1] to [-1,1]\n",
    "r1 = implementations.sigmoid(t1_std @ w1)\n",
    "r1_pred = np.sign(r1 - 0.5)\n",
    "r2 = implementations.sigmoid(t2_std @ w2)\n",
    "r2_pred = np.sign(r2 - 0.5)\n",
    "r3 = implementations.sigmoid(t3_std @ w3)\n",
    "r3_pred = np.sign(r3 - 0.5)\n",
    "\n",
    "#recreate the full prediction of the testing set\n",
    "pred4V = create_with_index(r0_pred, r1_pred, r2_pred, r3_pred, idx0, idx1, idx2, idx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "b6016c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples = len(test_data[0])\n",
    "#txTE = np.c_[np.ones(num_samples), test_data[1]]\n",
    "\n",
    "\n",
    "#rig_log\n",
    "#print(txTE.shape, w1.shape)\n",
    "#prediTE = implementations.sigmoid(txTE @ opt_wTR)\n",
    "#predi_tTE = np.sign(prediTE - 0.5)\n",
    "#print(predi_tTE.shape)\n",
    "\n",
    "#Store the prediction on a file\n",
    "ids_ = test_data[2]\n",
    "y_pred = pred4V\n",
    "name = \"logistic_ridge_10.csv\"\n",
    "create_csv_submission(ids_, y_pred, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "5874be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164333\n",
      "85667\n"
     ]
    }
   ],
   "source": [
    "#check if the training set has the same amount of 1 and -1 as ground true\n",
    "x_test = train_data[0]\n",
    "print(len(x_test[x_test == -1]))\n",
    "print(len(x_test[x_test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple test on training data\n",
    "test = 0\n",
    "\n",
    "for i in range(len(pred4V)):\n",
    "        if(pred4V[i] != np.array(train_data[0])[i]):\n",
    "            test += 1 \n",
    "        \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross testing + lambda testing\n",
    "\"\"\" Remarks for report: to find the best lambda, we manually tested them by narrowing our range.\n",
    "    First, 1-10 (1), 0.1-1 (1), 1-2 (ev. faire du '0.1-2')\n",
    "\"\"\"\n",
    "#form K subgroups randomly\n",
    "k_fold = 10 #to change when on better computer\n",
    "seed = 0\n",
    "k_indices = build_k_indices(y0, k_fold, seed)\n",
    "#take a range of lambda\n",
    "#lambdas = np.logspace(-5, 0, 15)\n",
    "lambdas = np.linspace(0.1,1,10)\n",
    "#print(lambdas)\n",
    "#initialize the variables\n",
    "rmse_tr_tmp = []\n",
    "rmse_te_tmp = []\n",
    "w_tr_tmp = []\n",
    "w_te_tmp = []\n",
    "opt_rmse_te = np.Inf\n",
    "opt_rmse_tr = np.Inf\n",
    "opt_lambda = 0\n",
    "opt_wTR = np.ones(len(y))\n",
    "for lambda_ in lambdas:\n",
    "    for k in range(k_fold):\n",
    "        # Put the kth group in test\n",
    "        te_indice = k_indices[k]\n",
    "        tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "        tr_indice = tr_indice.reshape(-1)\n",
    "        #create new x and y for test and training set\n",
    "        y_te = y[te_indice]\n",
    "        y_tr = y[tr_indice]\n",
    "        x_te = tx0[te_indice]\n",
    "        x_tr = tx0[tr_indice]\n",
    "        #print(lambda_)\n",
    "        loss_tr, w_tr = implementations.logistic_regression(y_tr, x_tr, np.ones(len(x_tr[0])), 10000, lambda_)\n",
    "        loss_te, w_te = implementations.logistic_regression(y_te, x_te, np.ones(len(x_te[0])), 10000, lambda_)\n",
    "        rmse_tr_tmp.append(loss_tr)\n",
    "        rmse_te_tmp.append(loss_te)\n",
    "        w_tr_tmp.append(w_tr)\n",
    "        w_te_tmp.append(w_te)\n",
    "    rmse_tr = np.mean(rmse_tr_tmp, axis=0)\n",
    "    rmse_te = np.mean(rmse_te_tmp, axis=0)\n",
    "    wTR = np.mean(w_tr_tmp, axis=0)\n",
    "    wTE = np.mean(w_te_tmp, axis=0)\n",
    "    if rmse_te < opt_rmse_te:\n",
    "        opt_rmse_te = rmse_te\n",
    "        opt_rmse_tr = rmse_tr\n",
    "        opt_lambda = lambda_\n",
    "        opt_wTR = wTR\n",
    "\n",
    "\n",
    "#return opt_rms_tr, opt_lambda, opt_wTR\n",
    "print(opt_rmse_tr, opt_lambda, opt_wTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cf1c4-7a05-4403-9550-45bf7c192e47",
   "metadata": {},
   "source": [
    "## Visualization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "ef94df50-c58e-4d54-b84c-3f89cb6bc128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the correlation values betweeny and each feature against the ids of the features\n",
    "def corrplot(ind, corr):\n",
    "    plt.scatter(ind, corr, )\n",
    "    plt.axhline(0, c='r', ls=':')\n",
    "    plt.title(\"Correlations coefficient of the different features of x\")\n",
    "    plt.ylabel(\"Correlation coefficients\")\n",
    "    plt.xlabel(\"Features' indices\")\n",
    "    plt.savefig('Corrplot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "#compute the correlation coeffincients between y and each of the features of x\n",
    "corr0 = []\n",
    "corr1 = []\n",
    "corr2 = []\n",
    "corr3 = []\n",
    "for i in range(len(tx0[1])):\n",
    "    feat0 = extract(tx0,i)\n",
    "    corr0.append(np.corrcoef(y0,feat0)[0,1])\n",
    "for i in range(len(tx1[1])):\n",
    "    feat1 = extract(tx1,i)\n",
    "    corr1.append(np.corrcoef(y1,feat1)[0,1])\n",
    "for i in range(len(tx2[1])):\n",
    "    feat2 = extract(tx2,i)\n",
    "    corr2.append(np.corrcoef(y2,feat2)[0,1])\n",
    "for i in range(len(tx3[1])):\n",
    "    feat3 = extract(tx3,i)\n",
    "    corr3.append(np.corrcoef(y3,feat3)[0,1])\n",
    "\n",
    "#get an array corresponding to the indices of each feature\n",
    "def listFrom1toN(n):\n",
    "    return list(range(0,n))\n",
    "ind0 = listFrom1toN(len(tx0[1]))\n",
    "ind1 = listFrom1toN(len(tx1[1]))\n",
    "ind2 = listFrom1toN(len(tx2[1]))\n",
    "ind3 = listFrom1toN(len(tx3[1]))\n",
    "\n",
    "#corrplot(ind0, corr0)\n",
    "#corrplot(ind1, corr1)\n",
    "#corrplot(ind2, corr2)\n",
    "corrplot(ind3, corr3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dafc7a-7dfe-4d4c-8b79-a2e37a5ea06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d'apres le graph 0: 2,5,7,8,9,10,13,16,18\n",
    "#d'apres le graph 1: 2,4,7,8,9,10,18,20,23\n",
    "#d'apres le graph 2: 2,4,5,6,7,9,10,11,12,13,14,20,24,27,30\n",
    "#d'apres le graph 3: 2,4,5,6,7,11,12,13,14,20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

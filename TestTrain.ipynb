{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0bb53f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e3e70-ad50-459c-96a7-373b34bcbd5e",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4cb6fecb-bb32-4b35-b3cb-6790a9c63b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y == \"b\")] = -1\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "def cleanning(data, y):\n",
    "    toDelete = []\n",
    "    mean = np.mean(data, axis=0)\n",
    "    var = np.var(data, axis=0)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if((data[i][j] < (mean[j] - 2 * var[j])) | (data[i][j] > (mean[j] + 2 * var[j]))):\n",
    "                toDelete = toDelete + [i]\n",
    "                break\n",
    "                \n",
    "    return np.delete(data, toDelete, axis=0), np.delete(y, toDelete, axis=0)\n",
    "\n",
    "#standardize the data\n",
    "def standardize(x):\n",
    "\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    \n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "#create a csv submission\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})\n",
    "            \n",
    "#Cross-validation implementation\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "86f72593-360d-4401-912a-47b212775529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5868/1588762953.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ids = x[:, 0].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_csv_data('train.csv', sub_sample=True)\n",
    "test_data = load_csv_data('test.csv', sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9a9227c3-7837-4fb0-820a-80ed8a274cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11365, 30)\n"
     ]
    }
   ],
   "source": [
    "print(test_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f68fe92f-6d59-4590-a7bb-c80ec3733d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1.,  1.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9aecfbda-bbab-43c9-9be7-2f40f7ce1867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 219.057,   72.461,  124.835, ..., -999.   , -999.   ,   50.396],\n",
       "       [  90.801,   27.787,   65.373, ..., -999.   , -999.   ,   62.766],\n",
       "       ...,\n",
       "       [ 142.347,    7.389,   99.212, ..., -999.   , -999.   ,   97.068],\n",
       "       [  78.162,   46.335,   60.136, ..., -999.   , -999.   ,   32.44 ],\n",
       "       [ 130.042,    4.073,   67.819, ..., -999.   , -999.   ,   51.037]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e101771a-325d-48fd-9842-c3cedd385f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100050, 100100, ..., 349850, 349900, 349950])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "00efb60e-e466-4420-9b5b-a1d2a578111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data[4] is ut of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a8405e33-8450-4d40-bd07-02668350241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_std = train_data\n",
    "# 1st column is 'y'\n",
    "# 2nd column is 'X'\n",
    "# 3rd column is 'ids'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6dc389ef-1b7f-4aad-9f24-d21a6ef95373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleanned, y_clean = cleanning(train_data[1], train_data[0])\n",
    "train_data_std1, mean1, std1 = standardize(train_data_cleanned) #see to what it corresponds\n",
    "#train_data_std2, mean2, std2 = standardize(train_data[2]) #see to what it corresponds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "066e38fd-506b-49c7-96ca-5a7ce233dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model_data(height, weight):   \n",
    "    \n",
    "    y = weight\n",
    "    x = height\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1c1c7253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 31)\n",
      "1.0\n",
      "0.0\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5036003103607855\n",
      "Current iteration=200, loss=0.4889633069053449\n",
      "Current iteration=300, loss=0.4850916139478635\n",
      "Current iteration=400, loss=0.4827336658520129\n",
      "Current iteration=500, loss=0.4811377684126328\n",
      "Current iteration=600, loss=0.48001677551827815\n",
      "Current iteration=700, loss=0.47921565795333687\n",
      "Current iteration=800, loss=0.47863779455377076\n",
      "Current iteration=900, loss=0.4782183435486517\n",
      "Current iteration=1000, loss=0.4779121400816231\n",
      "Current iteration=1100, loss=0.47768723765267657\n",
      "Current iteration=1200, loss=0.47752093388990363\n",
      "Current iteration=1300, loss=0.4773970704520704\n",
      "Current iteration=1400, loss=0.47730412216309287\n",
      "Current iteration=1500, loss=0.4772338377440826\n",
      "Current iteration=1600, loss=0.4771802791096098\n",
      "Current iteration=1700, loss=0.47713914657638595\n",
      "Current iteration=1800, loss=0.4771073061301039\n",
      "Current iteration=1900, loss=0.47708245790981624\n",
      "Current iteration=2000, loss=0.4770629028642596\n",
      "Current iteration=2100, loss=0.4770473776217402\n",
      "\n",
      "Current iteration=0, loss=inf\n",
      "\n",
      "Gradient Descent: loss=0.08131843648284638, w0=0.3538736895934569, w1=-0.0050525324091425\n",
      "\n",
      "SGD: loss=0.0932139955851746, w0=0.35965976088413315, w1=0.0024440794363187633\n",
      "\n",
      "ok\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.503852373489837\n",
      "Current iteration=200, loss=0.489787488214721\n",
      "Current iteration=300, loss=0.48622557864878074\n",
      "Current iteration=400, loss=0.4840611751377043\n",
      "Current iteration=500, loss=0.4825897263481966\n",
      "Current iteration=600, loss=0.4815466046838453\n",
      "Current iteration=700, loss=0.48079007847744626\n",
      "Current iteration=800, loss=0.480232999952226\n",
      "Current iteration=900, loss=0.47981809728398644\n",
      "Current iteration=1000, loss=0.479506273675251\n",
      "Current iteration=1100, loss=0.47927015125454886\n",
      "Current iteration=1200, loss=0.47909019492496013\n",
      "Current iteration=1300, loss=0.47895226091253734\n",
      "Current iteration=1400, loss=0.47884598867913886\n",
      "Current iteration=1500, loss=0.4787637162017747\n",
      "Current iteration=1600, loss=0.47869973184008835\n",
      "Current iteration=1700, loss=0.47864974896221674\n",
      "Current iteration=1800, loss=0.4786105315126192\n",
      "Current iteration=1900, loss=0.47857962388077896\n",
      "Current iteration=2000, loss=0.4785551540262322\n",
      "Current iteration=2100, loss=0.47853568876034797\n",
      "Current iteration=2200, loss=0.4785201265881334\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.4600959283135706\n",
      "Current iteration=200, loss=0.4341955538068728\n",
      "Current iteration=300, loss=0.4271853131713431\n",
      "Current iteration=400, loss=0.42341614264591476\n",
      "Current iteration=500, loss=0.42112458682721365\n",
      "Current iteration=600, loss=0.4196421522833144\n",
      "Current iteration=700, loss=0.4186405303256467\n",
      "Current iteration=800, loss=0.4179408067152155\n",
      "Current iteration=900, loss=0.4174387839101931\n",
      "Current iteration=1000, loss=0.4170706690467213\n",
      "Current iteration=1100, loss=0.4167958045627503\n",
      "Current iteration=1200, loss=0.41658739571987724\n",
      "Current iteration=1300, loss=0.4164272743644503\n",
      "Current iteration=1400, loss=0.4163028170708517\n",
      "Current iteration=1500, loss=0.41620506635700505\n",
      "Current iteration=1600, loss=0.4161275496125014\n",
      "Current iteration=1700, loss=0.41606551605505687\n",
      "Current iteration=1800, loss=0.4160154314625073\n",
      "Current iteration=1900, loss=0.41597463604851104\n",
      "Current iteration=2000, loss=0.4159411080792593\n",
      "Current iteration=2100, loss=0.41591329754927087\n",
      "Current iteration=2200, loss=0.4158900072279944\n",
      "Current iteration=2300, loss=0.4158703063417498\n",
      "Current iteration=2400, loss=0.4158534671279608\n",
      "Current iteration=2500, loss=0.4158389176708453\n",
      "Current iteration=2600, loss=0.4158262064906805\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5022880025311841\n",
      "Current iteration=200, loss=0.48817115489806745\n",
      "Current iteration=300, loss=0.48441864966238257\n",
      "Current iteration=400, loss=0.4821375258928867\n",
      "Current iteration=500, loss=0.48059805039695214\n",
      "Current iteration=600, loss=0.4795205489651062\n",
      "Current iteration=700, loss=0.47875406992099745\n",
      "Current iteration=800, loss=0.4782044285827508\n",
      "Current iteration=900, loss=0.4778082096023845\n",
      "Current iteration=1000, loss=0.4775211284934952\n",
      "Current iteration=1100, loss=0.4773118812039027\n",
      "Current iteration=1200, loss=0.47715831192705505\n",
      "Current iteration=1300, loss=0.47704475271266605\n",
      "Current iteration=1400, loss=0.4769601136209316\n",
      "Current iteration=1500, loss=0.4768965186461364\n",
      "Current iteration=1600, loss=0.47684834452505737\n",
      "Current iteration=1700, loss=0.4768115502051756\n",
      "Current iteration=1800, loss=0.47678321107279903\n",
      "Current iteration=1900, loss=0.4767611952247028\n",
      "Current iteration=2000, loss=0.4767439375836234\n",
      "Current iteration=2100, loss=0.4767302813511575\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.4913716619684936\n",
      "Current iteration=200, loss=0.46843460511230267\n",
      "Current iteration=300, loss=0.4621578118644337\n",
      "Current iteration=400, loss=0.4581115755440028\n",
      "Current iteration=500, loss=0.45519643035682067\n",
      "Current iteration=600, loss=0.4530192129511613\n",
      "Current iteration=700, loss=0.4513620910002724\n",
      "Current iteration=800, loss=0.45008396650954\n",
      "Current iteration=900, loss=0.4490874869232591\n",
      "Current iteration=1000, loss=0.44830333026597596\n",
      "Current iteration=1100, loss=0.4476811248569472\n",
      "Current iteration=1200, loss=0.44718370197499707\n",
      "Current iteration=1300, loss=0.4467832846090983\n",
      "Current iteration=1400, loss=0.44645888596878053\n",
      "Current iteration=1500, loss=0.4461944958218102\n",
      "Current iteration=1600, loss=0.4459777943836967\n",
      "Current iteration=1700, loss=0.44579922708988834\n",
      "Current iteration=1800, loss=0.4456513306121209\n",
      "Current iteration=1900, loss=0.4455282364389671\n",
      "Current iteration=2000, loss=0.44542530161003213\n",
      "Current iteration=2100, loss=0.44533883157438026\n",
      "Current iteration=2200, loss=0.44526587049650673\n",
      "Current iteration=2300, loss=0.4452040414125839\n",
      "Current iteration=2400, loss=0.4451514235493658\n",
      "Current iteration=2500, loss=0.44510645756631345\n",
      "Current iteration=2600, loss=0.44506787193111447\n",
      "Current iteration=2700, loss=0.44503462539719596\n",
      "Current iteration=2800, loss=0.44500586182619567\n",
      "Current iteration=2900, loss=0.44498087453001584\n",
      "Current iteration=3000, loss=0.4449590779937367\n",
      "Current iteration=3100, loss=0.44493998535052565\n",
      "Current iteration=3200, loss=0.44492319036089556\n",
      "Current iteration=3300, loss=0.4449083529355447\n",
      "Current iteration=3400, loss=0.44489518745819717\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5037815108861378\n",
      "Current iteration=200, loss=0.48901419795102036\n",
      "Current iteration=300, loss=0.48510999537050803\n",
      "Current iteration=400, loss=0.48273183046311313\n",
      "Current iteration=500, loss=0.4811188307230916\n",
      "Current iteration=600, loss=0.47998340320938365\n",
      "Current iteration=700, loss=0.4791706778812398\n",
      "Current iteration=800, loss=0.47858401176725046\n",
      "Current iteration=900, loss=0.47815839159211404\n",
      "Current iteration=1000, loss=0.477848336746234\n",
      "Current iteration=1100, loss=0.4776214714628692\n",
      "Current iteration=1200, loss=0.47745462463005794\n",
      "Current iteration=1300, loss=0.47733120182375277\n",
      "Current iteration=1400, loss=0.4772393160282136\n",
      "Current iteration=1500, loss=0.4771704398224141\n",
      "Current iteration=1600, loss=0.4771184377746964\n",
      "Current iteration=1700, loss=0.4770788771741449\n",
      "Current iteration=1800, loss=0.47704853974655353\n",
      "Current iteration=1900, loss=0.47702507648715997\n",
      "Current iteration=2000, loss=0.47700676346671717\n",
      "Current iteration=2100, loss=0.47699232857148305\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.4838863056107156\n",
      "Current iteration=200, loss=0.4660615982183935\n",
      "Current iteration=300, loss=0.4614825689018076\n",
      "Current iteration=400, loss=0.4587911040859582\n",
      "Current iteration=500, loss=0.4569771039381919\n",
      "Current iteration=600, loss=0.4556700046421022\n",
      "Current iteration=700, loss=0.4546838400904615\n",
      "Current iteration=800, loss=0.4539139058913092\n",
      "Current iteration=900, loss=0.45329660101270997\n",
      "Current iteration=1000, loss=0.4527910099839036\n",
      "Current iteration=1100, loss=0.4523695912420505\n",
      "Current iteration=1200, loss=0.4520131137979496\n",
      "Current iteration=1300, loss=0.45170773921326757\n",
      "Current iteration=1400, loss=0.4514432571652653\n",
      "Current iteration=1500, loss=0.45121197426698895\n",
      "Current iteration=1600, loss=0.4510079893665358\n",
      "Current iteration=1700, loss=0.4508267063058044\n",
      "Current iteration=1800, loss=0.45066449750143367\n",
      "Current iteration=1900, loss=0.4505184661846291\n",
      "Current iteration=2000, loss=0.4503862749005619\n",
      "Current iteration=2100, loss=0.4502660195721521\n",
      "Current iteration=2200, loss=0.4501561355683739\n",
      "Current iteration=2300, loss=0.4500553266833552\n",
      "Current iteration=2400, loss=0.44996251079636623\n",
      "Current iteration=2500, loss=0.4498767778606884\n",
      "Current iteration=2600, loss=0.44979735712662444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2700, loss=0.4497235913621385\n",
      "Current iteration=2800, loss=0.4496549164311195\n",
      "Current iteration=2900, loss=0.4495908450108459\n",
      "Current iteration=3000, loss=0.44953095353284334\n",
      "Current iteration=3100, loss=0.4494748716516191\n",
      "Current iteration=3200, loss=0.4494222737082194\n",
      "Current iteration=3300, loss=0.449372871776758\n",
      "Current iteration=3400, loss=0.4493264099734415\n",
      "Current iteration=3500, loss=0.4492826597771533\n",
      "Current iteration=3600, loss=0.4492414161640041\n",
      "Current iteration=3700, loss=0.4492024943995004\n",
      "Current iteration=3800, loss=0.44916572736405586\n",
      "Current iteration=3900, loss=0.4491309633126724\n",
      "Current iteration=4000, loss=0.4490980639893545\n",
      "Current iteration=4100, loss=0.4490669030324112\n",
      "Current iteration=4200, loss=0.4490373646191646\n",
      "Current iteration=4300, loss=0.449009342308427\n",
      "Current iteration=4400, loss=0.44898273804697264\n",
      "Current iteration=4500, loss=0.4489574613125181\n",
      "Current iteration=4600, loss=0.4489334283707954\n",
      "Current iteration=4700, loss=0.44891056162836185\n",
      "Current iteration=4800, loss=0.4488887890660924\n",
      "Current iteration=4900, loss=0.4488680437409538\n",
      "Current iteration=5000, loss=0.44884826334582356\n",
      "Current iteration=5100, loss=0.44882938981887105\n",
      "Current iteration=5200, loss=0.4488113689954523\n",
      "Current iteration=5300, loss=0.44879415029663783\n",
      "Current iteration=5400, loss=0.4487776864494498\n",
      "Current iteration=5500, loss=0.4487619332346759\n",
      "Current iteration=5600, loss=0.4487468492587695\n",
      "Current iteration=5700, loss=0.4487323957468847\n",
      "Current iteration=5800, loss=0.44871853635453435\n",
      "Current iteration=5900, loss=0.44870523699572873\n",
      "Current iteration=6000, loss=0.4486924656857556\n",
      "Current iteration=6100, loss=0.44868019239702295\n",
      "Current iteration=6200, loss=0.44866838892659217\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5015456518782548\n",
      "Current iteration=200, loss=0.4866408163950307\n",
      "Current iteration=300, loss=0.4826728474783852\n",
      "Current iteration=400, loss=0.4802730162036406\n",
      "Current iteration=500, loss=0.47866257918548605\n",
      "Current iteration=600, loss=0.4775411654769814\n",
      "Current iteration=700, loss=0.4767469422274434\n",
      "Current iteration=800, loss=0.4761794801062131\n",
      "Current iteration=900, loss=0.47577161924606837\n",
      "Current iteration=1000, loss=0.47547678859564535\n",
      "Current iteration=1100, loss=0.4752622765990944\n",
      "Current iteration=1200, loss=0.4751050528581049\n",
      "Current iteration=1300, loss=0.4749888982648333\n",
      "Current iteration=1400, loss=0.47490236975957606\n",
      "Current iteration=1500, loss=0.4748373614617643\n",
      "Current iteration=1600, loss=0.47478809840348446\n",
      "Current iteration=1700, loss=0.4747504382966904\n",
      "Current iteration=1800, loss=0.47472138835744976\n",
      "Current iteration=1900, loss=0.47469877042107195\n",
      "Current iteration=2000, loss=0.47468098778920514\n",
      "Current iteration=2100, loss=0.47466686188999524\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5115176332435699\n",
      "Current iteration=200, loss=0.4960796111275009\n",
      "Current iteration=300, loss=0.4920128324224469\n",
      "Current iteration=400, loss=0.4893457883912707\n",
      "Current iteration=500, loss=0.48733601150622\n",
      "Current iteration=600, loss=0.48575987145\n",
      "Current iteration=700, loss=0.4845031848583941\n",
      "Current iteration=800, loss=0.48349175184189913\n",
      "Current iteration=900, loss=0.4826721314641145\n",
      "Current iteration=1000, loss=0.48200415535864927\n",
      "Current iteration=1100, loss=0.4814570081721292\n",
      "Current iteration=1200, loss=0.4810067580283926\n",
      "Current iteration=1300, loss=0.48063465256566135\n",
      "Current iteration=1400, loss=0.48032589092473477\n",
      "Current iteration=1500, loss=0.48006871746480384\n",
      "Current iteration=1600, loss=0.47985374253529134\n",
      "Current iteration=1700, loss=0.47967342758489734\n",
      "Current iteration=1800, loss=0.47952169126086536\n",
      "Current iteration=1900, loss=0.4793936057338883\n",
      "Current iteration=2000, loss=0.4792851610011266\n",
      "Current iteration=2100, loss=0.4791930808468007\n",
      "Current iteration=2200, loss=0.479114678349252\n",
      "Current iteration=2300, loss=0.4790477418608156\n",
      "Current iteration=2400, loss=0.4789904446069161\n",
      "Current iteration=2500, loss=0.47894127269078424\n",
      "Current iteration=2600, loss=0.4788989675126138\n",
      "Current iteration=2700, loss=0.4788624795302494\n",
      "Current iteration=2800, loss=0.47883093098303753\n",
      "Current iteration=2900, loss=0.47880358572901005\n",
      "Current iteration=3000, loss=0.4787798247500398\n",
      "Current iteration=3100, loss=0.47875912619071337\n",
      "Current iteration=3200, loss=0.4787410490371291\n",
      "Current iteration=3300, loss=0.478725219728509\n",
      "Current iteration=3400, loss=0.47871132114007836\n",
      "Current iteration=3500, loss=0.4786990834896273\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5032277026758182\n",
      "Current iteration=200, loss=0.48783600096994484\n",
      "Current iteration=300, loss=0.483696440221325\n",
      "Current iteration=400, loss=0.4811892072956488\n",
      "Current iteration=500, loss=0.47949812863336405\n",
      "Current iteration=600, loss=0.47831337689993486\n",
      "Current iteration=700, loss=0.47746885298278596\n",
      "Current iteration=800, loss=0.4768609795065013\n",
      "Current iteration=900, loss=0.4764200296609386\n",
      "Current iteration=1000, loss=0.47609752728868826\n",
      "Current iteration=1100, loss=0.4758594823687133\n",
      "Current iteration=1200, loss=0.4756820420672552\n",
      "Current iteration=1300, loss=0.47554844267425594\n",
      "Current iteration=1400, loss=0.47544684974822743\n",
      "Current iteration=1500, loss=0.4753688485119579\n",
      "Current iteration=1600, loss=0.47530840245946554\n",
      "Current iteration=1700, loss=0.475261139303726\n",
      "Current iteration=1800, loss=0.47522386112489634\n",
      "Current iteration=1900, loss=0.4751942066045813\n",
      "Current iteration=2000, loss=0.4751704162904027\n",
      "Current iteration=2100, loss=0.47515116794526535\n",
      "Current iteration=2200, loss=0.47513545992079825\n",
      "Current iteration=2300, loss=0.4751225277360375\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.4892951324281596\n",
      "Current iteration=200, loss=0.4770648897153327\n",
      "Current iteration=300, loss=0.47372933594156824\n",
      "Current iteration=400, loss=0.4714573313236412\n",
      "Current iteration=500, loss=0.46979932666280516\n",
      "Current iteration=600, loss=0.46857304415313084\n",
      "Current iteration=700, loss=0.46765934076203\n",
      "Current iteration=800, loss=0.46697442234301606\n",
      "Current iteration=900, loss=0.4664582646692993\n",
      "Current iteration=1000, loss=0.4660674304482275\n",
      "Current iteration=1100, loss=0.4657702215413116\n",
      "Current iteration=1200, loss=0.46554333253871794\n",
      "Current iteration=1300, loss=0.4653695122092387\n",
      "Current iteration=1400, loss=0.4652359124777001\n",
      "Current iteration=1500, loss=0.4651329117653727\n",
      "Current iteration=1600, loss=0.46505326901202837\n",
      "Current iteration=1700, loss=0.46499151047659676\n",
      "Current iteration=1800, loss=0.46494348191492685\n",
      "Current iteration=1900, loss=0.46490601926942404\n",
      "Current iteration=2000, loss=0.464876704973955\n",
      "Current iteration=2100, loss=0.4648536865737577\n",
      "Current iteration=2200, loss=0.4648355410163544\n",
      "Current iteration=2300, loss=0.4648211726291572\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.505587961948639\n",
      "Current iteration=200, loss=0.49126206317043297\n",
      "Current iteration=300, loss=0.4875911505273023\n",
      "Current iteration=400, loss=0.48535914337504427\n",
      "Current iteration=500, loss=0.48385004196898734\n",
      "Current iteration=600, loss=0.4827916271480025\n",
      "Current iteration=700, loss=0.48203694315429857\n",
      "Current iteration=800, loss=0.4814941577796883\n",
      "Current iteration=900, loss=0.48110145762313516\n",
      "Current iteration=1000, loss=0.480815742411646\n",
      "Current iteration=1100, loss=0.4806065608110773\n",
      "Current iteration=1200, loss=0.4804523273301101\n",
      "Current iteration=1300, loss=0.48033772715567424\n",
      "Current iteration=1400, loss=0.48025187445908335\n",
      "Current iteration=1500, loss=0.4801870038206176\n",
      "Current iteration=1600, loss=0.4801375476906754\n",
      "Current iteration=1700, loss=0.4800994902442131\n",
      "Current iteration=1800, loss=0.48006991600961324\n",
      "Current iteration=1900, loss=0.48004669424290736\n",
      "Current iteration=2000, loss=0.4800282574217328\n",
      "Current iteration=2100, loss=0.48001344497007636\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.46089847854012844\n",
      "Current iteration=200, loss=0.4416284736722596\n",
      "Current iteration=300, loss=0.43579539598957334\n",
      "Current iteration=400, loss=0.43214960697418003\n",
      "Current iteration=500, loss=0.42961288505153783\n",
      "Current iteration=600, loss=0.42777379169540136\n",
      "Current iteration=700, loss=0.4264054813271145\n",
      "Current iteration=800, loss=0.42536636824605795\n",
      "Current iteration=900, loss=0.42456324021335773\n",
      "Current iteration=1000, loss=0.42393280427026697\n",
      "Current iteration=1100, loss=0.4234310665907322\n",
      "Current iteration=1200, loss=0.4230268216963593\n",
      "Current iteration=1300, loss=0.4226975233902698\n",
      "Current iteration=1400, loss=0.422426603686774\n",
      "Current iteration=1500, loss=0.42220169711150535\n",
      "Current iteration=1600, loss=0.422013443039322\n",
      "Current iteration=1700, loss=0.42185466383698667\n",
      "Current iteration=1800, loss=0.4217197914965078\n",
      "Current iteration=1900, loss=0.42160446126049655\n",
      "Current iteration=2000, loss=0.42150521924463935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2100, loss=0.4214193090730815\n",
      "Current iteration=2200, loss=0.4213445140905104\n",
      "Current iteration=2300, loss=0.42127903922384685\n",
      "Current iteration=2400, loss=0.4212214215165131\n",
      "Current iteration=2500, loss=0.42117046166539246\n",
      "Current iteration=2600, loss=0.42112517112941084\n",
      "Current iteration=2700, loss=0.4210847309140635\n",
      "Current iteration=2800, loss=0.42104845920275774\n",
      "Current iteration=2900, loss=0.4210157857560295\n",
      "Current iteration=3000, loss=0.42098623153388054\n",
      "Current iteration=3100, loss=0.4209593923813953\n",
      "Current iteration=3200, loss=0.4209349258983322\n",
      "Current iteration=3300, loss=0.4209125408201076\n",
      "Current iteration=3400, loss=0.42089198839146846\n",
      "Current iteration=3500, loss=0.4208730553298463\n",
      "Current iteration=3600, loss=0.4208555580631277\n",
      "Current iteration=3700, loss=0.42083933799369927\n",
      "Current iteration=3800, loss=0.42082425759235625\n",
      "Current iteration=3900, loss=0.42081019716582097\n",
      "Current iteration=4000, loss=0.4207970521729865\n",
      "Current iteration=4100, loss=0.42078473098964864\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.501654357350478\n",
      "Current iteration=200, loss=0.4866738788621492\n",
      "Current iteration=300, loss=0.48267099045595047\n",
      "Current iteration=400, loss=0.4802016250410057\n",
      "Current iteration=500, loss=0.47852342372794615\n",
      "Current iteration=600, loss=0.4773435629525707\n",
      "Current iteration=700, loss=0.47650038155189506\n",
      "Current iteration=800, loss=0.4758921374958657\n",
      "Current iteration=900, loss=0.47545029994235005\n",
      "Current iteration=1000, loss=0.4751271708171091\n",
      "Current iteration=1100, loss=0.4748891371453923\n",
      "Current iteration=1200, loss=0.47471242307710393\n",
      "Current iteration=1300, loss=0.4745801735813173\n",
      "Current iteration=1400, loss=0.4744803969936611\n",
      "Current iteration=1500, loss=0.4744045155632066\n",
      "Current iteration=1600, loss=0.47434635197688224\n",
      "Current iteration=1700, loss=0.47430142412561593\n",
      "Current iteration=1800, loss=0.47426645456861877\n",
      "Current iteration=1900, loss=0.47423902823080677\n",
      "Current iteration=2000, loss=0.4742173521819456\n",
      "Current iteration=2100, loss=0.4742000858599751\n",
      "Current iteration=2200, loss=0.4741862201521446\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5011918937007555\n",
      "Current iteration=200, loss=0.48225263559824927\n",
      "Current iteration=300, loss=0.4774300641062514\n",
      "Current iteration=400, loss=0.4749106220070272\n",
      "Current iteration=500, loss=0.4734158505747221\n",
      "Current iteration=600, loss=0.47247560179526277\n",
      "Current iteration=700, loss=0.47186180714155357\n",
      "Current iteration=800, loss=0.4714496427108376\n",
      "Current iteration=900, loss=0.47116632985432266\n",
      "Current iteration=1000, loss=0.4709676631192167\n",
      "Current iteration=1100, loss=0.470825931945187\n",
      "Current iteration=1200, loss=0.47072329496851567\n",
      "Current iteration=1300, loss=0.47064799070912305\n",
      "Current iteration=1400, loss=0.47059210013874303\n",
      "Current iteration=1500, loss=0.47055018935075704\n",
      "Current iteration=1600, loss=0.47051846598766445\n",
      "Current iteration=1700, loss=0.4704942431966388\n",
      "Current iteration=1800, loss=0.47047559190062643\n",
      "Current iteration=1900, loss=0.4704611108435329\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5065949606829191\n",
      "Current iteration=200, loss=0.4917052217217601\n",
      "Current iteration=300, loss=0.48748002185453926\n",
      "Current iteration=400, loss=0.48484882297645326\n",
      "Current iteration=500, loss=0.4830546892744902\n",
      "Current iteration=600, loss=0.48179659276881637\n",
      "Current iteration=700, loss=0.48090513964648807\n",
      "Current iteration=800, loss=0.48026996483289625\n",
      "Current iteration=900, loss=0.47981477894947366\n",
      "Current iteration=1000, loss=0.4794862209743153\n",
      "Current iteration=1100, loss=0.4792471032591634\n",
      "Current iteration=1200, loss=0.4790715786112845\n",
      "Current iteration=1300, loss=0.47894163963648795\n",
      "Current iteration=1400, loss=0.4788446603258201\n",
      "Current iteration=1500, loss=0.47877171187313694\n",
      "Current iteration=1600, loss=0.47871642143289905\n",
      "Current iteration=1700, loss=0.4786741990044153\n",
      "Current iteration=1800, loss=0.4786417104936164\n",
      "Current iteration=1900, loss=0.47861651521334514\n",
      "Current iteration=2000, loss=0.47859681388626873\n",
      "Current iteration=2100, loss=0.4785812716212816\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.4540769552002803\n",
      "Current iteration=200, loss=0.4344524966560868\n",
      "Current iteration=300, loss=0.43041014361728525\n",
      "Current iteration=400, loss=0.4282368477839554\n",
      "Current iteration=500, loss=0.4268502300950923\n",
      "Current iteration=600, loss=0.42591067685365797\n",
      "Current iteration=700, loss=0.4252495091320754\n",
      "Current iteration=800, loss=0.4247701845329608\n",
      "Current iteration=900, loss=0.42441361836310326\n",
      "Current iteration=1000, loss=0.4241421321324385\n",
      "Current iteration=1100, loss=0.4239309813244457\n",
      "Current iteration=1200, loss=0.4237635314116556\n",
      "Current iteration=1300, loss=0.42362837344019444\n",
      "Current iteration=1400, loss=0.42351753892362326\n",
      "Current iteration=1500, loss=0.4234253660018115\n",
      "Current iteration=1600, loss=0.4233477642233207\n",
      "Current iteration=1700, loss=0.4232817295554923\n",
      "Current iteration=1800, loss=0.423225019688435\n",
      "Current iteration=1900, loss=0.4231759337603665\n",
      "Current iteration=2000, loss=0.42313316108521876\n",
      "Current iteration=2100, loss=0.42309567605007536\n",
      "Current iteration=2200, loss=0.42306266425368544\n",
      "Current iteration=2300, loss=0.42303347000489216\n",
      "Current iteration=2400, loss=0.4230075585698323\n",
      "Current iteration=2500, loss=0.42298448870139244\n",
      "Current iteration=2600, loss=0.4229638924060882\n",
      "Current iteration=2700, loss=0.4229454598549552\n",
      "Current iteration=2800, loss=0.42292892798723963\n",
      "Current iteration=2900, loss=0.4229140717926109\n",
      "Current iteration=3000, loss=0.42290069755710624\n",
      "Current iteration=3100, loss=0.42288863756479506\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5038632169550284\n",
      "Current iteration=200, loss=0.4893210770992261\n",
      "Current iteration=300, loss=0.4855524571616086\n",
      "Current iteration=400, loss=0.4832815194104596\n",
      "Current iteration=500, loss=0.4817448380542879\n",
      "Current iteration=600, loss=0.4806618387868298\n",
      "Current iteration=700, loss=0.4798841642418202\n",
      "Current iteration=800, loss=0.47932005605251354\n",
      "Current iteration=900, loss=0.47890806120400126\n",
      "Current iteration=1000, loss=0.47860533873307665\n",
      "Current iteration=1100, loss=0.47838149690368575\n",
      "Current iteration=1200, loss=0.478214844355321\n",
      "Current iteration=1300, loss=0.47808986461533387\n",
      "Current iteration=1400, loss=0.4779954328923503\n",
      "Current iteration=1500, loss=0.47792354400792253\n",
      "Current iteration=1600, loss=0.47786840840660233\n",
      "Current iteration=1700, loss=0.4778258132841257\n",
      "Current iteration=1800, loss=0.47779267241522316\n",
      "Current iteration=1900, loss=0.4777667088702178\n",
      "Current iteration=2000, loss=0.4777462307556757\n",
      "Current iteration=2100, loss=0.4777299719679792\n",
      "Current iteration=2200, loss=0.4777169784593807\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.48439952870805203\n",
      "Current iteration=200, loss=0.4654284844893814\n",
      "Current iteration=300, loss=0.4596276201231971\n",
      "Current iteration=400, loss=0.4560071129966643\n",
      "Current iteration=500, loss=0.4535953410806418\n",
      "Current iteration=600, loss=0.4519381434140255\n",
      "Current iteration=700, loss=0.4507723470606488\n",
      "Current iteration=800, loss=0.4499362498753177\n",
      "Current iteration=900, loss=0.4493266508336553\n",
      "Current iteration=1000, loss=0.44887569705036545\n",
      "Current iteration=1100, loss=0.44853766780265125\n",
      "Current iteration=1200, loss=0.44828111075566907\n",
      "Current iteration=1300, loss=0.4480840041302683\n",
      "Current iteration=1400, loss=0.44793069509330297\n",
      "Current iteration=1500, loss=0.4478099137830747\n",
      "Current iteration=1600, loss=0.44771345654870054\n",
      "Current iteration=1700, loss=0.44763529568566846\n",
      "Current iteration=1800, loss=0.4475709669559943\n",
      "Current iteration=1900, loss=0.4475171416454958\n",
      "Current iteration=2000, loss=0.4474713234348805\n",
      "Current iteration=2100, loss=0.44743163107631895\n",
      "Current iteration=2200, loss=0.4473966409315841\n",
      "Current iteration=2300, loss=0.44736527182608293\n",
      "Current iteration=2400, loss=0.44733670016927346\n",
      "Current iteration=2500, loss=0.44731029694966273\n",
      "Current iteration=2600, loss=0.44728558068517127\n",
      "Current iteration=2700, loss=0.4472621821055615\n",
      "Current iteration=2800, loss=0.44723981752254177\n",
      "Current iteration=2900, loss=0.4472182686728171\n",
      "Current iteration=3000, loss=0.44719736740981114\n",
      "Current iteration=3100, loss=0.4471769840442677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3200, loss=0.4471570184419157\n",
      "Current iteration=3300, loss=0.4471373932116335\n",
      "Current iteration=3400, loss=0.447118048483513\n",
      "Current iteration=3500, loss=0.4470989378992763\n",
      "Current iteration=3600, loss=0.44708002552926746\n",
      "Current iteration=3700, loss=0.4470612834990041\n",
      "Current iteration=3800, loss=0.44704269016004344\n",
      "Current iteration=3900, loss=0.44702422867902086\n",
      "Current iteration=4000, loss=0.44700588594837926\n",
      "Current iteration=4100, loss=0.44698765174484456\n",
      "Current iteration=4200, loss=0.4469695180788943\n",
      "Current iteration=4300, loss=0.44695147869159474\n",
      "Current iteration=4400, loss=0.44693352866523356\n",
      "Current iteration=4500, loss=0.44691566412188366\n",
      "Current iteration=4600, loss=0.44689788198995206\n",
      "Current iteration=4700, loss=0.44688017982332023\n",
      "Current iteration=4800, loss=0.4468625556611875\n",
      "Current iteration=4900, loss=0.4468450079194267\n",
      "Current iteration=5000, loss=0.44682753530635033\n",
      "Current iteration=5100, loss=0.4468101367573864\n",
      "Current iteration=5200, loss=0.44679281138441085\n",
      "Current iteration=5300, loss=0.44677555843644207\n",
      "Current iteration=5400, loss=0.4467583772691449\n",
      "Current iteration=5500, loss=0.446741267321166\n",
      "Current iteration=5600, loss=0.44672422809576917\n",
      "Current iteration=5700, loss=0.4467072591465822\n",
      "Current iteration=5800, loss=0.44669036006653373\n",
      "Current iteration=5900, loss=0.44667353047926456\n",
      "Current iteration=6000, loss=0.4466567700324613\n",
      "Current iteration=6100, loss=0.4466400783926804\n",
      "Current iteration=6200, loss=0.44662345524132996\n",
      "Current iteration=6300, loss=0.44660690027154976\n",
      "Current iteration=6400, loss=0.44659041318578924\n",
      "Current iteration=6500, loss=0.44657399369392603\n",
      "Current iteration=6600, loss=0.4465576415118059\n",
      "Current iteration=6700, loss=0.4465413563601088\n",
      "Current iteration=6800, loss=0.4465251379634679\n",
      "Current iteration=6900, loss=0.4465089860497863\n",
      "Current iteration=7000, loss=0.4464929003497048\n",
      "Current iteration=7100, loss=0.4464768805961903\n",
      "Current iteration=7200, loss=0.4464609265242151\n",
      "Current iteration=7300, loss=0.4464450378705075\n",
      "Current iteration=7400, loss=0.44642921437335864\n",
      "Current iteration=7500, loss=0.44641345577247166\n",
      "Current iteration=7600, loss=0.4463977618088447\n",
      "Current iteration=7700, loss=0.4463821322246797\n",
      "Current iteration=7800, loss=0.4463665667633114\n",
      "Current iteration=7900, loss=0.4463510651691519\n",
      "Current iteration=8000, loss=0.4463356271876466\n",
      "Current iteration=8100, loss=0.44632025256524116\n",
      "Current iteration=8200, loss=0.4463049410493541\n",
      "Current iteration=8300, loss=0.4462896923883557\n",
      "Current iteration=8400, loss=0.44627450633155163\n",
      "Current iteration=8500, loss=0.446259382629169\n",
      "Current iteration=8600, loss=0.4462443210323457\n",
      "Current iteration=8700, loss=0.44622932129312215\n",
      "Current iteration=8800, loss=0.4462143831644331\n",
      "Current iteration=8900, loss=0.446199506400103\n",
      "Current iteration=9000, loss=0.4461846907548389\n",
      "Current iteration=9100, loss=0.44616993598422816\n",
      "Current iteration=9200, loss=0.4461552418447331\n",
      "Current iteration=9300, loss=0.44614060809368866\n",
      "Current iteration=9400, loss=0.4461260344892981\n",
      "Current iteration=9500, loss=0.4461115207906315\n",
      "Current iteration=9600, loss=0.4460970667576219\n",
      "Current iteration=9700, loss=0.44608267215106345\n",
      "Current iteration=9800, loss=0.44606833673260915\n",
      "Current iteration=9900, loss=0.44605406026476735\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5007707905806226\n",
      "Current iteration=200, loss=0.4857361070337414\n",
      "Current iteration=300, loss=0.4817844120165701\n",
      "Current iteration=400, loss=0.4793819792115521\n",
      "Current iteration=500, loss=0.4777531215083169\n",
      "Current iteration=600, loss=0.47660507307411804\n",
      "Current iteration=700, loss=0.47578064229213224\n",
      "Current iteration=800, loss=0.4751821401891565\n",
      "Current iteration=900, loss=0.47474416308749945\n",
      "Current iteration=1000, loss=0.4744212443530473\n",
      "Current iteration=1100, loss=0.4741812543566742\n",
      "Current iteration=1200, loss=0.4740013524810307\n",
      "Current iteration=1300, loss=0.4738652569391965\n",
      "Current iteration=1400, loss=0.47376132499664303\n",
      "Current iteration=1500, loss=0.47368119243174855\n",
      "Current iteration=1600, loss=0.473618813455583\n",
      "Current iteration=1700, loss=0.4735697866474815\n",
      "Current iteration=1800, loss=0.4735308827022588\n",
      "Current iteration=1900, loss=0.4734997131714871\n",
      "Current iteration=2000, loss=0.4734744972168972\n",
      "Current iteration=2100, loss=0.4734538964570076\n",
      "Current iteration=2200, loss=0.4734368972467019\n",
      "Current iteration=2300, loss=0.47342272615650705\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5105337217706516\n",
      "Current iteration=200, loss=0.49543321870530543\n",
      "Current iteration=300, loss=0.4912591046625295\n",
      "Current iteration=400, loss=0.48857112466321895\n",
      "Current iteration=500, loss=0.48664824328181266\n",
      "Current iteration=600, loss=0.4852132582902137\n",
      "Current iteration=700, loss=0.4841157455801116\n",
      "Current iteration=800, loss=0.48326219771811463\n",
      "Current iteration=900, loss=0.4825901996805528\n",
      "Current iteration=1000, loss=0.48205614614065034\n",
      "Current iteration=1100, loss=0.48162856091063255\n",
      "Current iteration=1200, loss=0.48128415528727275\n",
      "Current iteration=1300, loss=0.4810053646192653\n",
      "Current iteration=1400, loss=0.4807787385218934\n",
      "Current iteration=1500, loss=0.48059385035632707\n",
      "Current iteration=1600, loss=0.4804425362849259\n",
      "Current iteration=1700, loss=0.48031835111515553\n",
      "Current iteration=1800, loss=0.48021617113627674\n",
      "Current iteration=1900, loss=0.4801318992357719\n",
      "Current iteration=2000, loss=0.48006224277287896\n",
      "Current iteration=2100, loss=0.48000454419554517\n",
      "Current iteration=2200, loss=0.4799566505176341\n",
      "Current iteration=2300, loss=0.47991681183070434\n",
      "Current iteration=2400, loss=0.4798836017734243\n",
      "Current iteration=2500, loss=0.4798558547827168\n",
      "Current iteration=2600, loss=0.47983261628975993\n",
      "Current iteration=2700, loss=0.47981310298261787\n",
      "Current iteration=2800, loss=0.4797966709535841\n",
      "Current iteration=2900, loss=0.47978279006164964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/Downloads/implementations.py:114: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1.0 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "import implementations\n",
    "\n",
    "_, tx = build_model_data(train_data_std1, train_data_std1)\n",
    "\n",
    "print(tx.shape)\n",
    "\n",
    "_, tc = build_model_data(test_data[1], test_data[1])\n",
    "\n",
    "y = (y_clean + 1)/2\n",
    "\n",
    "print(max(y))\n",
    "print(min(y))\n",
    "    \n",
    "loss, w = implementations.logistic_regression(y, tx, np.ones(len(tx[0])), 10000, 0.2)\n",
    "print()\n",
    "loss1, w1 = implementations.reg_logistic_regression(y, tx, 1, np.ones(len(tx[0])), 10000, 0.2)\n",
    "print()\n",
    "loss2, w2 = implementations.mean_squared_error_gd(y, tx, np.ones(len(tx[0])), 10000, 0.02)\n",
    "print()\n",
    "loss3, w3 = implementations.mean_squared_error_sgd(y, tx, np.ones(len(tx[0])), 10000, 0.002)\n",
    "print()\n",
    "w4, mse = implementations.least_squares(y, tx)\n",
    "print(\"ok\")\n",
    "w5, mse = implementations.ridge_regression(y, tx, 0.03)\n",
    "\n",
    "#form K subgroups randomly\n",
    "k_fold = 10\n",
    "seed = 0\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "#Pattern with method\n",
    "rmse_tr_tmp = []\n",
    "rmse_te_tmp = []\n",
    "w_tr_tmp = []\n",
    "w_te_tmp = []\n",
    "for k in range(k_fold):\n",
    "    # Put the kth group in test\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = tx[te_indice]\n",
    "    x_tr = tx[tr_indice]\n",
    "    loss_tr, w_tr = implementations.logistic_regression(y_tr, x_tr, np.ones(len(x_tr[0])), 10000, 0.2)\n",
    "    loss_te, w_te = implementations.logistic_regression(y_te, x_te, np.ones(len(x_te[0])), 10000, 0.2)\n",
    "    rmse_tr_tmp.append(loss_tr)\n",
    "    rmse_te_tmp.append(loss_te)\n",
    "    w_tr_tmp.append(w_tr)\n",
    "    w_te_tmp.append(w_te)\n",
    "rmse_tr = np.mean(rmse_tr_tmp, axis=0)\n",
    "rmse_te = np.mean(rmse_te_tmp, axis=0)\n",
    "wTR = np.mean(w_tr_tmp, axis=0)\n",
    "wTE = np.mean(w_te_tmp, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "predi = implementations.sigmoid(tc @ w)\n",
    "predi_t = np.sign(predi - 0.5)\n",
    "\n",
    "predi1 = implementations.sigmoid(tc @ w1)\n",
    "predi_t1 = np.sign(predi1 - 0.5)\n",
    "\n",
    "predi2 = tc @ w2\n",
    "predi_t2 = np.sign(predi2 - 0.5)\n",
    "\n",
    "predi3 = tc @ w3\n",
    "predi_t3 = np.sign(predi3 - 0.5)\n",
    "\n",
    "predi4 = tc @ w4\n",
    "predi_t4 = np.sign(predi4 - 0.5)\n",
    "\n",
    "predi5 = tc @ w5\n",
    "predi_t5 = np.sign(predi5 - 0.5)\n",
    "\n",
    "predi6 = tc @ wTR\n",
    "predi_t6 = np.sign(predi6 - 0.5)\n",
    "\n",
    "predi7 = tc @ wTE\n",
    "predi_t7 = np.sign(predi7 - 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "219f38d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11365,)\n",
      "1.0\n",
      "1.0\n",
      "test 0 : 0.2788385393752749\n",
      "test 1 : 0.27989441267047954\n",
      "test 2 : 0.2346678398592169\n",
      "test 3 : 0.1782666080070392\n",
      "test 4 : 0.2942366915970084\n",
      "test 5 : 0.280686317641883\n",
      "test 6 : 0.2791025076990761\n",
      "test 7 : 0.28438187417509897\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0].shape)\n",
    "print(max(test_data[0]))\n",
    "print(min(test_data[0]))\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t)):\n",
    "    if(predi_t[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 0 : \" + str(1 - test/len(predi_t)))\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t1)):\n",
    "    if(predi_t1[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 1 : \" + str(1 - test/len(predi_t1)))\n",
    "\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t2)):\n",
    "    if(predi_t2[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 2 : \" + str(1 - test/len(predi_t2)))\n",
    "\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t3)):\n",
    "    if(predi_t3[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 3 : \" + str(1 - test/len(predi_t3)))\n",
    "\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t4)):\n",
    "    if(predi_t4[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 4 : \" + str(1 - test/len(predi_t4)))\n",
    "\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t5)):\n",
    "    if(predi_t5[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 5 : \" + str(1 - test/len(predi_t5)))\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t6)):\n",
    "    if(predi_t6[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 6 : \" + str(1 - test/len(predi_t6)))\n",
    "\n",
    "test = 0\n",
    "\n",
    "for i in range(len(predi_t7)):\n",
    "    if(predi_t7[i] != test_data[0][i]):\n",
    "        test += 1\n",
    "\n",
    "print(\"test 7 : \" + str(1 - test/len(predi_t7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b59fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

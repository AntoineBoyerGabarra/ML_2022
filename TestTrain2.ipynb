{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bb53f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e3e70-ad50-459c-96a7-373b34bcbd5e",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4cb6fecb-bb32-4b35-b3cb-6790a9c63b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y == \"b\")] = -1\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "##############################\n",
    "\n",
    "#standardize the data\n",
    "def standardize(x):\n",
    "\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "##############################\n",
    "\n",
    "#extract each feature of x\n",
    "def extract(lst, nb):\n",
    "    return [item[nb] for item in lst]\n",
    "\n",
    "##############################\n",
    "\n",
    "#create subplot for each feature\n",
    "def subplot(y,x,n):\n",
    "    feat = x[:,n] #extract(x,n)\n",
    "    ax.plot(feat,y,'o')\n",
    "    ax.set_title(f'Feature '+ str(n))\n",
    "    \n",
    "##############################\n",
    "\n",
    "#compute the number of undefined values in a certain feature\n",
    "def nb_undef_feat(x, n):\n",
    "    feat = x[:,n] #extract(x,n)\n",
    "    unique, frequency = np.unique(feat, return_counts = True) #ligne Ã  changer\n",
    "    \n",
    "    if(unique[0] == -999): \n",
    "        return int(frequency[0]) \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "##############################\n",
    "\n",
    "#remove a column from the data\n",
    "def remove_feat(x, n):\n",
    "    d = np.delete(d,n, axis = 1)\n",
    "    return d\n",
    "\n",
    "##############################\n",
    "\n",
    "#remove a feature from a data set that has more than \n",
    "#only put x as argument\n",
    "def remove_feat_w_undef(x):\n",
    "    xdel = []\n",
    "    #xinter= x[0]\n",
    "    for i in range(len(x[0])):\n",
    "        nb = nb_undef_feat(x, i)\n",
    "        if(nb > np.double(0.6*len(x))):\n",
    "            xdel = xdel + [i]\n",
    "        elif(nb > 0):\n",
    "            taille = len(x[:, i])\n",
    "            if(taille % 2 == 0):\n",
    "                index1 = int((taille + nb)/2 - 0.5)\n",
    "                index2 = int((taille + nb)/2 + 0.5)\n",
    "                x_sorted = np.sort(x[:, i])\n",
    "                xmedian = (x_sorted[index1] + x_sorted[index2]) / 2.0\n",
    "            else:\n",
    "                index = int((taille + nb)/2)\n",
    "                xmedian = (np.sort(x[:, i]))[index]\n",
    "\n",
    "            x[:,i][x[:,i] == -999] = xmedian\n",
    " \n",
    "    x = np.delete(x,xdel,axis =1)\n",
    "    return x, xdel\n",
    "       \n",
    "##############################\n",
    "\n",
    "def Multi_Model(tx):\n",
    "#change so that we also keep the indices\n",
    "    tx0 = []\n",
    "    tx1 = []\n",
    "    tx2 = []\n",
    "    tx3 = []\n",
    "    index0 = []\n",
    "    index1 = []\n",
    "    index2 = []\n",
    "    index3 = []\n",
    "    \n",
    "    for i in range(len(tx)):\n",
    "        if(tx[i][22]==0):\n",
    "            tx0 = tx0 + [tx[i]]\n",
    "            index0 = index0 + [i]\n",
    "        else:\n",
    "            if(tx[i][22]==1):\n",
    "                tx1 = tx1 + [tx[i]]\n",
    "                index1 = index1 + [i]\n",
    "            else:\n",
    "                if(tx[i][22]==2):\n",
    "                    tx2 = tx2 + [tx[i]]\n",
    "                    index2 = index2 + [i]\n",
    "                else:\n",
    "                    if(tx[i][22]==3):\n",
    "                        tx3 = tx3 + [tx[i]]\n",
    "                        index3 = index3 + [i]\n",
    "    \n",
    "    return np.array(tx0), np.array(tx1), np.array(tx2), np.array(tx3), index0, index1, index2, index3\n",
    "\n",
    "##############################\n",
    "              \n",
    "#create a csv submission\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})\n",
    "            \n",
    "######################################\n",
    "def build_model_data(height, weight):   \n",
    "    \n",
    "    y = weight\n",
    "    x = height\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx\n",
    "######################################\n",
    "#Cross-validation implementation\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "##########################################\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x),1)) #careful ! \n",
    "    for deg in range(1,degree+1) :\n",
    "        poly = np.c_[poly, np.power(x,deg)]\n",
    "    return poly\n",
    "##########################################\n",
    "def id_outliers(x):\n",
    "    for i in range(len(x[0])):\n",
    "        m = np.mean(x[:,i])\n",
    "        std = np.std(x[:,i])\n",
    "        nb = nb_undef_feat(x, i)\n",
    "        \n",
    "        taille = len(x[:, i])\n",
    "        if(taille % 2 == 0):\n",
    "            index1 = int((taille + nb)/2 - 0.5)\n",
    "            index2 = int((taille + nb)/2 + 0.5)\n",
    "            x_sorted = np.sort(x[:, i])\n",
    "            xmedian = (x_sorted[index1] + x_sorted[index2]) / 2.0\n",
    "        else:\n",
    "            index = int((taille + nb)/2)\n",
    "            xmedian = (np.sort(x[:, i]))[index]\n",
    "             \n",
    "        print(len(x[x[:,i] > m + 2*std]))\n",
    "        print(len(x[x[:,i] < m - 2*std]))\n",
    "        x[x[:,i] > m + 3*std] = xmedian\n",
    "        x[x[:,i] < m - 3*std] = xmedian\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b36fe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add more useful features => convert angles in x and y axis [to do after the splitting of classes (+cleaning)?]\n",
    "#to be converted : PRI_tau_phi [15], PRI_lep_phi [18], PRI_met_phi[20], PRI_jet_leading_phi [25] (NDA if 0),PRI_jet_subleading_phi [28] (NDA if 0/1) \n",
    "def to_carthesian(phi):\n",
    "    x = np.cos(phi)\n",
    "    y = np.sin(phi)\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "\n",
    "#This data must be used with the already DIVIDED DATA, but not cleaned\n",
    "def data_to_carthesian(data):\n",
    "    if(data[1][22]==0):\n",
    "        #PRI_tau_phi \n",
    "        colA, colB = to_carthesian(data[:,15])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_lep_phi\n",
    "        colA, colB = to_carthesian(data[:,18])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_met_phi\n",
    "        colA, colB = to_carthesian(data[:,20])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #data.append(to_carthesian(data[:,11]),to_carthesian(data[:,14]),to_carthesian(data[:,16]))\n",
    "        toDelete = [15,18,20]\n",
    "        data = np.delete(data, toDelete, axis=1)\n",
    "        return data\n",
    "        \n",
    "    #offsetting of four after cleaning\n",
    "    elif(data[1][22]==1):\n",
    "        #PRI_tau_phi \n",
    "        colA, colB = to_carthesian(data[:,15])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_lep_phi\n",
    "        colA, colB = to_carthesian(data[:,18])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_met_phi\n",
    "        colA, colB = to_carthesian(data[:,20])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_jet_leading_phi\n",
    "        colA, colB = to_carthesian(data[:,25])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #data.append(to_carthesian(data[:,11]),to_carthesian(data[:,14]),to_carthesian(data[:,16]), to_carthesian(data[:,21]))\n",
    "        toDelete = [15,18,20,25]\n",
    "        data = np.delete(data, toDelete, axis=1)\n",
    "        return data\n",
    "    elif(data[1][22]==2 or data[1][22]==3):\n",
    "        #PRI_tau_phi \n",
    "        colA, colB = to_carthesian(data[:,15])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_lep_phi\n",
    "        colA, colB = to_carthesian(data[:,18])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_met_phi\n",
    "        colA, colB = to_carthesian(data[:,20])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_jet_leading_phi\n",
    "        colA, colB = to_carthesian(data[:,25])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #PRI_jet_subleading_phi\n",
    "        colA, colB = to_carthesian(data[:,28])\n",
    "        colModA = np.array([colA]).T\n",
    "        colModB = np.array([colB]).T\n",
    "        data = np.append(data,colModA, axis = 1)\n",
    "        data = np.append(data,colModB, axis = 1)\n",
    "        #data.append(to_carthesian(data[:,15]),to_carthesian(data[:,18]),to_carthesian(data[:,20]), to_carthesian(data[:,25]), to_carthesian(data[:,28]))\n",
    "        toDelete = [15,18,20,25,28]\n",
    "        data = np.delete(data, toDelete, axis=1)\n",
    "        return data\n",
    "    else:\n",
    "        print(\"ERROR: there's a mistake in your indexes, darling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86f72593-360d-4401-912a-47b212775529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109715/167336792.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ids = x[:, 0].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_csv_data('train.csv', sub_sample=False)\n",
    "test_data = load_csv_data('test.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a9227c3-7837-4fb0-820a-80ed8a274cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1., -1., -1., ...,  1., -1., -1.]), array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
      "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
      "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
      "       ...,\n",
      "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
      "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
      "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]]), array([100000, 100001, 100002, ..., 349997, 349998, 349999]))\n"
     ]
    }
   ],
   "source": [
    "type(train_data)\n",
    "len(train_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f68fe92f-6d59-4590-a7bb-c80ec3733d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9aecfbda-bbab-43c9-9be7-2f40f7ce1867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e101771a-325d-48fd-9842-c3cedd385f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100001, 100002, ..., 349997, 349998, 349999])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00efb60e-e466-4420-9b5b-a1d2a578111f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data[4] is out of range\n",
    "train_data[1][1]\n",
    "len(train_data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e36b1be-422c-4795-b5f6-4a0fcd7d4e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8405e33-8450-4d40-bd07-02668350241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_std = train_data\n",
    "# 1st column is 'y'\n",
    "# 2nd column is 'X'\n",
    "# 3rd column is 'ids'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6dc389ef-1b7f-4aad-9f24-d21a6ef95373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_std1, mean1, std1 = standardize(train_data[1]) #see to what it corresponds\n",
    "#train_data_std2, mean2, std2 = standardize(train_data[2]) #see to what it corresponds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a51aa03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_with_index(x, idx):\n",
    "    x_fin = []\n",
    "    \n",
    "    for i in range(len(idx)):\n",
    "        x_fin += [x[idx[i]]]\n",
    "    \n",
    "    return np.array(x_fin)\n",
    "\n",
    "def create_with_index(x0, x1, x2, x3, idx0, idx1, idx2, idx3):\n",
    "    total_length = len(x0) + len(x1) + len(x2) + len(x3)\n",
    "    x_fin = np.repeat(x0[0], total_length).reshape(total_length,)\n",
    "    \n",
    "    print(x0.shape)\n",
    "    print(len(idx0))\n",
    "    print(x_fin.shape)\n",
    "    \n",
    "    for i in range(len(x0)):\n",
    "        x_fin[idx0[i]] = x0[i]\n",
    "    for i in range(len(x1)):\n",
    "        x_fin[idx1[i]] = x1[i]\n",
    "    for i in range(len(x2)):\n",
    "        x_fin[idx2[i]] = x2[i]\n",
    "    for i in range(len(x3)):\n",
    "        x_fin[idx3[i]] = x3[i]\n",
    "        \n",
    "    return x_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ee8f07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nzero, un, deux, trois, idx0, idx1, idx2, idx3 = Multi_Model(train_data[1])\\nx_fin = create_with_index(zero, un, deux, trois, idx0, idx1, idx2, idx3)\\n\\ntest = True\\n\\nfor i in range(len(x_fin)):\\n    for j in range(len(x_fin[0])):\\n        if(x_fin[i,j] != np.array(train_data[1])[i,j]):\\n            print(i)\\n            test = False\\n            break\\n        \\nprint(test)\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "zero, un, deux, trois, idx0, idx1, idx2, idx3 = Multi_Model(train_data[1])\n",
    "x_fin = create_with_index(zero, un, deux, trois, idx0, idx1, idx2, idx3)\n",
    "\n",
    "test = True\n",
    "\n",
    "for i in range(len(x_fin)):\n",
    "    for j in range(len(x_fin[0])):\n",
    "        if(x_fin[i,j] != np.array(train_data[1])[i,j]):\n",
    "            print(i)\n",
    "            test = False\n",
    "            break\n",
    "        \n",
    "print(test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e617e-d43c-414a-ac03-0b3e6488ffdc",
   "metadata": {},
   "source": [
    "## Removing columns with too many undefined the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f699be7d-c88b-43f4-8a87-5b2d1ecb71ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(99913, 20)\n",
      "[[160.937   68.768  103.235  ...   0.725    1.158   46.226 ]\n",
      " [112.4055 162.172  125.953  ...   2.053   -2.028   44.251 ]\n",
      " [154.916   10.418   94.714  ...  -0.715   -1.724   30.638 ]\n",
      " ...\n",
      " [112.4055  78.256   79.699  ...  -0.852   -0.706   78.984 ]\n",
      " [133.457   77.54    88.989  ...  -1.234    2.521   70.969 ]\n",
      " [105.457   60.526   75.839  ...   1.8     -0.166   41.992 ]]\n",
      "[[ 1.38470e+02  5.16550e+01  9.78270e+01 ...  1.24000e+00 -2.47500e+00\n",
      "   1.13497e+02]\n",
      " [ 1.48754e+02  2.88620e+01  1.07782e+02 ...  1.31000e-01 -2.76700e+00\n",
      "   1.79877e+02]\n",
      " [ 1.41481e+02  7.36000e-01  1.11581e+02 ... -7.98000e-01 -2.78500e+00\n",
      "   2.78009e+02]\n",
      " ...\n",
      " [ 1.19934e+02  2.00780e+01  8.87510e+01 ... -1.72500e+00 -2.75600e+00\n",
      "   1.12938e+02]\n",
      " [ 1.26151e+02  2.90230e+01  9.52580e+01 ... -5.99000e-01 -2.52500e+00\n",
      "   1.93099e+02]\n",
      " [ 2.17020e+02  4.71560e+01  6.28240e+01 ... -5.80000e-02 -1.13700e+00\n",
      "   1.74176e+02]]\n",
      "[[ 8.974400e+01  1.355000e+01  5.914900e+01 ...  2.240000e-01\n",
      "   3.106000e+00  1.936600e+02]\n",
      " [ 1.147440e+02  1.028600e+01  7.571200e+01 ...  1.773000e+00\n",
      "  -2.079000e+00  1.656400e+02]\n",
      " [ 1.216810e+02  6.041000e+00  7.320200e+01 ... -1.257000e+00\n",
      "  -6.090000e-01  2.534610e+02]\n",
      " ...\n",
      " [ 1.075345e+02  8.387100e+01  1.708100e+01 ...  3.070000e+00\n",
      "   1.612000e+00  2.718330e+02]\n",
      " [ 1.075345e+02  3.808300e+01  7.499700e+02 ...  5.150000e-01\n",
      "   4.160000e-01  2.035690e+02]\n",
      " [ 1.300750e+02  3.918000e+00  6.678100e+01 ...  5.780000e-01\n",
      "  -2.215000e+00  5.460660e+02]]\n",
      "inside\n",
      "3941\n",
      "277\n",
      "2831\n",
      "0\n",
      "1710\n",
      "3\n",
      "884\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#load data : already done\n",
    "####\n",
    "\n",
    "#separate the data in 4 according to feature index 22\n",
    "zero, un, deux, trois, idx0_train, idx1_train, idx2_train, idx3_train = Multi_Model(train_data[1])\n",
    "print(train_data[1].shape)\n",
    "\n",
    "y0 = get_with_index(y, idx0_train)\n",
    "y1 = get_with_index(y, idx1_train)\n",
    "y2 = get_with_index(y, idx2_train)\n",
    "y3 = get_with_index(y, idx3_train)\n",
    "\n",
    "#zero = data_to_carthesian(zero)\n",
    "#un = data_to_carthesian(un)\n",
    "#deux = data_to_carthesian(deux)\n",
    "#trois = data_to_carthesian(trois)\n",
    "\n",
    "#drop columns in each of the four subdivisions if they have columns with more than 60% of undefined data\n",
    "zero_new, xdel0 = remove_feat_w_undef(zero)\n",
    "print(zero_new.shape)\n",
    "un_new, xdel1 = remove_feat_w_undef(un)\n",
    "print(un_new)\n",
    "deux_new, xdel2 = remove_feat_w_undef(deux)\n",
    "print(deux_new)\n",
    "trois_new, xdel3 = remove_feat_w_undef(trois)\n",
    "print(trois_new)\n",
    "\n",
    "print(\"inside\")\n",
    "#for each subdivision : see correlation plots\n",
    "#for each column of each subdivision print the percent of -999 present \n",
    "\n",
    "#drop columns with little correlation\n",
    "\n",
    "zero_std = id_outliers(zero_new)\n",
    "un_std = id_outliers(un_new)\n",
    "deux_std = id_outliers(deux_new)\n",
    "trois_std = id_outliers(trois_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "fbc315c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import implementations\n",
    "\n",
    "tx0 = build_poly(zero_std, 1)\n",
    "tx1 = build_poly(un_std, 1)\n",
    "tx2 = build_poly(deux_std, 1)\n",
    "tx3 = build_poly(trois_std, 1)\n",
    "\n",
    "#standardize the cleaned data\n",
    "tx0_std, _, _ = standardize(tx0)\n",
    "\n",
    "tx1_std, _, _ = standardize(tx1)\n",
    "\n",
    "tx2_std, _, _ = standardize(tx2)\n",
    "\n",
    "tx3_std, _, _ = standardize(tx3)\n",
    "\n",
    "#tx0 = np.delete(tx0, [1,3,4,6,11,12,14,15,17,19,20], axis = 1)\n",
    "#tx1 = np.delete(tx1, [1,3,5,6,11,12,13,14,15,16,17,19,21,22], axis = 1)\n",
    "#tx2 = np.delete(tx2, [1,3,8,15,16,17,18,19,21,22,23,25,26,28,29], axis = 1)\n",
    "#tx3 = np.delete(tx3, [1,3,8,9,10,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30], axis = 1)\n",
    "\n",
    "#_, tc = build_model_data(test_data[1], test_data[1])\n",
    "\n",
    "y0_bis = (y0 + 1)/2\n",
    "y1_bis = (y1 + 1)/2\n",
    "y2_bis = (y2 + 1)/2\n",
    "y3_bis = (y3 + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11f60c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d'apres le graph 0: 2,5,7,8,9,10,13,16,18\n",
    "#d'apres le graph 1: 2,4,7,8,9,10,18,20,23\n",
    "#d'apres le graph 2: 2,4,5,6,7,9,10,11,12,13,14,20,24,27,30\n",
    "#d'apres le graph 3: 2,4,5,6,7,11,12,13,14,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "262519a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1, t2, t3, idx0, idx1, idx2, idx3 = Multi_Model(test_data[1])\n",
    "\n",
    "#t0 = data_to_carthesian(t0)\n",
    "#t1 = data_to_carthesian(t1)\n",
    "#t2 = data_to_carthesian(t2)\n",
    "#t3 = data_to_carthesian(t3)\n",
    "\n",
    "t0_cleaned = np.delete(t0, xdel0, axis = 1)\n",
    "t1_cleaned = np.delete(t1, xdel1, axis = 1)\n",
    "t2_cleaned = np.delete(t2, xdel2, axis = 1)\n",
    "t3_cleaned = np.delete(t3, xdel3, axis = 1)\n",
    "\n",
    "t0_cleaned = build_poly(t0_cleaned, 1)\n",
    "t1_cleaned = build_poly(t1_cleaned, 1)\n",
    "t2_cleaned = build_poly(t2_cleaned, 1)\n",
    "t3_cleaned = build_poly(t3_cleaned, 1)\n",
    "\n",
    "t0_std, _, _ = standardize(t0_cleaned)\n",
    "\n",
    "t1_std, _, _ = standardize(t1_cleaned)\n",
    "\n",
    "t2_std, _, _ = standardize(t2_cleaned)\n",
    "\n",
    "t3_std, _, _ = standardize(t3_cleaned)\n",
    "\n",
    "#t0_cleaned = np.delete(t0_cleaned, [1,3,4,6,11,12,14,15,17,19,20], axis = 1)\n",
    "#t1_cleaned = np.delete(t1_cleaned, [1,3,5,6,11,12,13,14,15,16,17,19,21,22], axis = 1)\n",
    "#t2_cleaned = np.delete(t2_cleaned, [1,3,8,15,16,17,18,19,21,22,23,25,26,28,29], axis = 1)\n",
    "#t3_cleaned = np.delete(t3_cleaned, [1,3,8,9,10,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "729fafc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  2.33217769e+00  1.30167760e+00  1.69499457e+00\n",
      "  -5.78033237e-01 -5.24166760e-01 -5.78033237e-01  1.03715881e+00\n",
      "  -5.44521412e-01 -6.28025912e-01 -1.05521083e-01 -6.12416095e-01\n",
      "   5.44956742e-01 -5.65861705e-01  2.23935460e-01  2.16021756e+00\n",
      "  -5.97746076e-01 -5.97746076e-01 -5.98827672e-01 -5.74849703e-01\n",
      "  -5.82912930e-01 -6.15221530e-01 -6.18207451e-01 -5.87414287e-01\n",
      "   5.43905278e+00  1.69436457e+00  2.87300659e+00  3.34122423e-01\n",
      "   2.74750792e-01  3.34122423e-01  1.07569840e+00  2.96503568e-01\n",
      "   3.94416547e-01  1.11346991e-02  3.75053473e-01  2.96977851e-01\n",
      "   3.20199470e-01  5.01470902e-02  4.66653989e+00  3.57300371e-01\n",
      "   3.57300371e-01  3.58594581e-01  3.30452181e-01  3.39787483e-01\n",
      "   3.78497531e-01  3.82180452e-01  3.45055544e-01  1.26848375e+01\n",
      "   2.20551640e+00  4.86973057e+00 -1.93133866e-01 -1.44015233e-01\n",
      "  -1.93133866e-01  1.11567008e+00 -1.61452541e-01 -2.47703811e-01\n",
      "  -1.17494551e-03 -2.29688783e-01  1.61840082e-01 -1.81188618e-01\n",
      "   1.12297117e-02  1.00807414e+01 -2.13574894e-01 -2.13574894e-01\n",
      "  -2.14736358e-01 -1.89960338e-01 -1.98066517e-01 -2.32859830e-01\n",
      "  -2.36266803e-01 -2.02690556e-01]\n",
      " [ 1.00000000e+00  1.95696902e+00  1.05237896e+00  2.40003715e-01\n",
      "  -4.82173830e-01 -5.65838784e-01 -4.82173830e-01  6.83405049e-01\n",
      "  -5.68910319e-01 -6.30157650e-01 -3.03372344e-02 -6.07235744e-01\n",
      "   1.15973286e-01 -6.19521885e-01  4.68580954e-01  2.71825134e+00\n",
      "  -5.97746076e-01 -5.97746076e-01 -5.83587545e-01 -6.15772438e-01\n",
      "  -5.76459313e-01 -5.89243872e-01 -6.19758800e-01 -5.91354385e-01\n",
      "   3.82972776e+00  1.10750147e+00  5.76017834e-02  2.32491602e-01\n",
      "   3.20173529e-01  2.32491602e-01  4.67042462e-01  3.23658951e-01\n",
      "   3.97098663e-01  9.20347792e-04  3.68735249e-01  1.34498032e-02\n",
      "   3.83807367e-01  2.19568110e-01  7.38889034e+00  3.57300371e-01\n",
      "   3.57300371e-01  3.40574423e-01  3.79175696e-01  3.32305339e-01\n",
      "   3.47208340e-01  3.84100970e-01  3.49700008e-01  7.49465860e+00\n",
      "   1.16551124e+00  1.38246420e-02 -1.12101366e-01 -1.81166600e-01\n",
      "  -1.12101366e-01  3.19179177e-01 -1.84132917e-01 -2.50234760e-01\n",
      "  -2.79208067e-05 -2.23909223e-01  1.55981788e-03 -2.37777063e-01\n",
      "   1.02885435e-01  2.00848611e+01 -2.13574894e-01 -2.13574894e-01\n",
      "  -1.98754991e-01 -2.33485943e-01 -1.91560508e-01 -2.04590387e-01\n",
      "  -2.38049956e-01 -2.06796633e-01]\n",
      " [ 1.00000000e+00  1.95696902e+00  7.35827447e-01  9.62845996e-01\n",
      "  -8.34014449e-02 -5.38492951e-01 -8.34014449e-02  5.62514921e-01\n",
      "  -5.71110822e-01 -6.28576038e-01 -6.10067436e-02 -6.34627421e-01\n",
      "   2.57755895e-02 -5.90686129e-01  4.73555007e-01  1.24535848e+00\n",
      "  -5.97746076e-01 -5.97746076e-01 -6.19153119e-01 -5.89551429e-01\n",
      "  -5.86182102e-01 -5.77954960e-01 -5.82912930e-01 -6.15221530e-01\n",
      "   3.82972776e+00  5.41442031e-01  9.27072411e-01  6.95580102e-03\n",
      "   2.89974658e-01  6.95580102e-03  3.16423037e-01  3.26167571e-01\n",
      "   3.95107836e-01  3.72182276e-03  4.02751964e-01  6.64381014e-04\n",
      "   3.48910103e-01  2.24254345e-01  1.55091774e+00  3.57300371e-01\n",
      "   3.57300371e-01  3.83350585e-01  3.47570888e-01  3.43609457e-01\n",
      "   3.34031936e-01  3.39787483e-01  3.78497531e-01  7.49465860e+00\n",
      "   3.98407907e-01  8.92627959e-01 -5.80123856e-04 -1.56149309e-01\n",
      "  -5.80123856e-04  1.77992680e-01 -1.86277829e-01 -2.48355318e-01\n",
      "  -2.27056287e-04 -2.55597440e-01  1.71248123e-05 -2.06096358e-01\n",
      "   1.06196768e-01  1.93144855e+00 -2.13574894e-01 -2.13574894e-01\n",
      "  -2.37352710e-01 -2.04910914e-01 -2.01417713e-01 -1.93055414e-01\n",
      "  -1.98066517e-01 -2.32859830e-01]\n",
      " [ 1.00000000e+00  1.57871174e+00 -1.53932147e-01  9.79556065e-01\n",
      "  -2.88208668e-01 -5.20613865e-01 -2.88208668e-01  6.82648627e-01\n",
      "  -5.74847092e-01 -5.65334502e-01  4.27836432e-02 -6.48426408e-01\n",
      "   4.21189079e-02 -6.17779821e-01 -3.19244927e-01  1.98596523e+00\n",
      "  -5.97746076e-01 -5.97746076e-01 -6.19281488e-01 -5.89894976e-01\n",
      "  -5.75821023e-01 -6.04432315e-01 -5.81958068e-01 -5.81128237e-01\n",
      "   2.49233077e+00  2.36951059e-02  9.59530084e-01  8.30642362e-02\n",
      "   2.71038796e-01  8.30642362e-02  4.66009147e-01  3.30449179e-01\n",
      "   3.19603099e-01  1.83044012e-03  4.20456807e-01  1.77400240e-03\n",
      "   3.81651907e-01  1.01917324e-01  3.94405791e+00  3.57300371e-01\n",
      "   3.57300371e-01  3.83509561e-01  3.47976083e-01  3.31569850e-01\n",
      "   3.65338424e-01  3.38675193e-01  3.37710028e-01  3.93467186e+00\n",
      "  -3.64743854e-03  9.39913513e-01 -2.39398329e-02 -1.41106555e-01\n",
      "  -2.39398329e-02  3.18120504e-01 -1.89957750e-01 -1.80682659e-01\n",
      "   7.83128970e-05 -2.72635297e-01  7.47190438e-05 -2.35776847e-01\n",
      "  -3.25365886e-02  7.83276189e+00 -2.13574894e-01 -2.13574894e-01\n",
      "  -2.37500372e-01 -2.05269343e-01 -1.90924890e-01 -2.20822349e-01\n",
      "  -1.97094761e-01 -1.96252833e-01]\n",
      " [ 1.00000000e+00  1.95696902e+00  1.06996006e+00  1.02583539e+00\n",
      "  -4.26313147e-01 -5.51329218e-01 -4.26313147e-01  1.31027331e+00\n",
      "  -5.76291172e-01 -6.30088884e-01  3.87964613e-01 -5.59122665e-01\n",
      "   3.24539702e-01 -5.63592437e-01  3.35840201e-01  1.68080591e+00\n",
      "  -5.97746076e-01 -5.97746076e-01 -6.17985988e-01 -5.86986907e-01\n",
      "  -5.79319529e-01 -5.84112436e-01 -5.98323731e-01 -6.20660701e-01\n",
      "   3.82972776e+00  1.14481453e+00  1.05233825e+00  1.81742899e-01\n",
      "   3.03963906e-01  1.81742899e-01  1.71681615e+00  3.32111515e-01\n",
      "   3.97012001e-01  1.50516541e-01  3.12618155e-01  1.05326018e-01\n",
      "   3.17636435e-01  1.12788641e-01  2.82510851e+00  3.57300371e-01\n",
      "   3.57300371e-01  3.81906681e-01  3.44553630e-01  3.35611117e-01\n",
      "   3.41187338e-01  3.57991287e-01  3.85219706e-01  7.49465860e+00\n",
      "   1.22490582e+00  1.07952582e+00 -7.74793871e-02 -1.67584183e-01\n",
      "  -7.74793871e-02  2.24949839e+00 -1.91392934e-01 -2.50152849e-01\n",
      "   5.83950918e-02 -1.74791896e-01  3.41824745e-02 -1.79017492e-01\n",
      "   3.78789597e-02  4.74845908e+00 -2.13574894e-01 -2.13574894e-01\n",
      "  -2.36012977e-01 -2.02248469e-01 -1.94426074e-01 -1.99291768e-01\n",
      "  -2.14194682e-01 -2.39090732e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tx0[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "8757bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.44297028885050294\n",
      "Current iteration=200, loss=0.4353344868694791\n",
      "Current iteration=300, loss=0.4331592940458485\n",
      "Current iteration=400, loss=0.4323291120530589\n",
      "Current iteration=500, loss=0.43192266972318405\n",
      "Current iteration=600, loss=0.43169054933748097\n",
      "Current iteration=700, loss=0.4315423260537713\n",
      "Current iteration=800, loss=0.4314384307301127\n",
      "Current iteration=900, loss=0.4313595472170336\n",
      "Current iteration=1000, loss=0.43129545504241984\n",
      "Current iteration=1100, loss=0.43124043315235255\n",
      "Current iteration=1200, loss=0.4311911359869185\n",
      "Current iteration=1300, loss=0.4311455162665644\n",
      "Current iteration=1400, loss=0.43110231717617603\n",
      "Current iteration=1500, loss=0.43106074642555386\n",
      "Current iteration=1600, loss=0.43102030300248084\n",
      "Current iteration=1700, loss=0.43098066809328817\n",
      "Current iteration=1800, loss=0.43094161479613385\n",
      "Current iteration=1900, loss=0.4309030285600843\n",
      "Current iteration=2000, loss=0.4308647955371069\n",
      "Current iteration=2100, loss=0.4308268784602681\n",
      "Current iteration=2200, loss=0.4307892289747544\n",
      "Current iteration=2300, loss=0.4307518117295087\n",
      "Current iteration=2400, loss=0.43071461485032153\n",
      "Current iteration=2500, loss=0.4306776317590115\n",
      "Current iteration=2600, loss=0.43064083723478364\n",
      "Current iteration=2700, loss=0.43060423077765575\n",
      "Current iteration=2800, loss=0.430567806042274\n",
      "Current iteration=2900, loss=0.43053155777247165\n",
      "Current iteration=3000, loss=0.43049548145949296\n",
      "Current iteration=3100, loss=0.43045956599589635\n",
      "Current iteration=3200, loss=0.43042382205661867\n",
      "Current iteration=3300, loss=0.43038823913720253\n",
      "Current iteration=3400, loss=0.43035280712739366\n",
      "Current iteration=3500, loss=0.43031753707138654\n",
      "Current iteration=3600, loss=0.4302824191143696\n",
      "Current iteration=3700, loss=0.4302474436932516\n",
      "Current iteration=3800, loss=0.43021262193077636\n",
      "Current iteration=3900, loss=0.43017793764890516\n",
      "Current iteration=4000, loss=0.4301433952480263\n",
      "Current iteration=4100, loss=0.43010899902543986\n",
      "Current iteration=4200, loss=0.43007473329372503\n",
      "Current iteration=4300, loss=0.43004060253142373\n",
      "Current iteration=4400, loss=0.43000660456512496\n",
      "Current iteration=4500, loss=0.42997273728161917\n",
      "Current iteration=4600, loss=0.4299389986253599\n",
      "Current iteration=4700, loss=0.42990538659615846\n",
      "Current iteration=4800, loss=0.42987189924705665\n",
      "Current iteration=4900, loss=0.4298385346823434\n",
      "Current iteration=5000, loss=0.42980529105568943\n",
      "Current iteration=5100, loss=0.4297721603148955\n",
      "Current iteration=5200, loss=0.42973915325299916\n",
      "Current iteration=5300, loss=0.4297062618687817\n",
      "Current iteration=5400, loss=0.42967347836198594\n",
      "Current iteration=5500, loss=0.4296408073215086\n",
      "Current iteration=5600, loss=0.42960825321463897\n",
      "Current iteration=5700, loss=0.42957580234820225\n",
      "Current iteration=5800, loss=0.42954345930606974\n",
      "Current iteration=5900, loss=0.4295112285528093\n",
      "Current iteration=6000, loss=0.4294790967350867\n",
      "Current iteration=6100, loss=0.429447068425278\n",
      "Current iteration=6200, loss=0.42941514225137456\n",
      "Current iteration=6300, loss=0.42938331687306897\n",
      "Current iteration=6400, loss=0.42935158523027134\n",
      "Current iteration=6500, loss=0.42931995758017016\n",
      "Current iteration=6600, loss=0.4292884268846387\n",
      "Current iteration=6700, loss=0.42925698628021147\n",
      "Current iteration=6800, loss=0.4292256458861131\n",
      "Current iteration=6900, loss=0.42919439885697513\n",
      "Current iteration=7000, loss=0.429163238514078\n",
      "Current iteration=7100, loss=0.4291321693418219\n",
      "Current iteration=7200, loss=0.42910119024302146\n",
      "Current iteration=7300, loss=0.4290703055699894\n",
      "Current iteration=7400, loss=0.42903950338380414\n",
      "Current iteration=7500, loss=0.4290087881167907\n",
      "Current iteration=7600, loss=0.42897815876078055\n",
      "Current iteration=7700, loss=0.42894761432827916\n",
      "Current iteration=7800, loss=0.4289171485965848\n",
      "Current iteration=7900, loss=0.42888677116130686\n",
      "Current iteration=8000, loss=0.4288564758049282\n",
      "Current iteration=8100, loss=0.4288262564618584\n",
      "Current iteration=8200, loss=0.4287961225815453\n",
      "Current iteration=8300, loss=0.42876606301214215\n",
      "Current iteration=8400, loss=0.4287360871056742\n",
      "Current iteration=8500, loss=0.4287061838740587\n",
      "Current iteration=8600, loss=0.4286763575802801\n",
      "Current iteration=8700, loss=0.42864660741341604\n",
      "Current iteration=8800, loss=0.4286169325778324\n",
      "Current iteration=8900, loss=0.42858733229278817\n",
      "Current iteration=9000, loss=0.42855780579205205\n",
      "Current iteration=9100, loss=0.4285283523235341\n",
      "Current iteration=9200, loss=0.4284989663451329\n",
      "Current iteration=9300, loss=0.42846965676935994\n",
      "Current iteration=9400, loss=0.4284404133081644\n",
      "Current iteration=9500, loss=0.42841124477628645\n",
      "Current iteration=9600, loss=0.4283821410330635\n",
      "Current iteration=9700, loss=0.4283531061453812\n",
      "Current iteration=9800, loss=0.4283241394496902\n",
      "Current iteration=9900, loss=0.42829524029406224\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5669959511868794\n",
      "Current iteration=200, loss=0.5816907673461812\n",
      "Current iteration=300, loss=0.5688769635453583\n",
      "Current iteration=400, loss=0.5629327401721492\n",
      "Current iteration=500, loss=0.5590739079338578\n",
      "Current iteration=600, loss=0.5564678604991387\n",
      "Current iteration=700, loss=0.5547971307288043\n",
      "Current iteration=800, loss=0.5538733424850286\n",
      "Current iteration=900, loss=0.5534433566323137\n",
      "Current iteration=1000, loss=0.5532308152816245\n",
      "Current iteration=1100, loss=0.5530910755752894\n",
      "Current iteration=1200, loss=0.5529832335064763\n",
      "Current iteration=1300, loss=0.5528954801473738\n",
      "Current iteration=1400, loss=0.5528217107151541\n",
      "Current iteration=1500, loss=0.5527578836913689\n",
      "Current iteration=1600, loss=0.5527012161285223\n",
      "Current iteration=1700, loss=0.5526497659049614\n",
      "Current iteration=1800, loss=0.5526021625309501\n",
      "Current iteration=1900, loss=0.5525574288499606\n",
      "Current iteration=2000, loss=0.5525148614013764\n",
      "Current iteration=2100, loss=0.5524739491959887\n",
      "Current iteration=2200, loss=0.5524343179517848\n",
      "Current iteration=2300, loss=0.5523956913995972\n",
      "Current iteration=2400, loss=0.5523578641593974\n",
      "Current iteration=2500, loss=0.5523206825426639\n",
      "Current iteration=2600, loss=0.5522840308395326\n",
      "Current iteration=2700, loss=0.5522478214385553\n",
      "Current iteration=2800, loss=0.5522119876497749\n",
      "Current iteration=2900, loss=0.5521764784517335\n",
      "Current iteration=3000, loss=0.5521412546194758\n",
      "Current iteration=3100, loss=0.5521062858518715\n",
      "Current iteration=3200, loss=0.5520715486276027\n",
      "Current iteration=3300, loss=0.5520370245962612\n",
      "Current iteration=3400, loss=0.552002699365029\n",
      "Current iteration=3500, loss=0.551968561579592\n",
      "Current iteration=3600, loss=0.5519346022251401\n",
      "Current iteration=3700, loss=0.5519008140928345\n",
      "Current iteration=3800, loss=0.5518671913712632\n",
      "Current iteration=3900, loss=0.5518337293327089\n",
      "Current iteration=4000, loss=0.551800424091609\n",
      "Current iteration=4100, loss=0.551767272418174\n",
      "Current iteration=4200, loss=0.5517342715942778\n",
      "Current iteration=4300, loss=0.5517014193018331\n",
      "Current iteration=4400, loss=0.5516687135361901\n",
      "Current iteration=4500, loss=0.5516361525388523\n",
      "Current iteration=4600, loss=0.5516037347451345\n",
      "Current iteration=4700, loss=0.5515714587433944\n",
      "Current iteration=4800, loss=0.5515393232432488\n",
      "Current iteration=4900, loss=0.5515073270507647\n",
      "Current iteration=5000, loss=0.5514754690490811\n",
      "Current iteration=5100, loss=0.5514437481832549\n",
      "Current iteration=5200, loss=0.5514121634484019\n",
      "Current iteration=5300, loss=0.5513807138804069\n",
      "Current iteration=5400, loss=0.5513493985486381\n",
      "Current iteration=5500, loss=0.5513182165502286\n",
      "Current iteration=5600, loss=0.5512871670055826\n",
      "Current iteration=5700, loss=0.5512562490548368\n",
      "Current iteration=5800, loss=0.5512254618550729\n",
      "Current iteration=5900, loss=0.5511948045781145\n",
      "Current iteration=6000, loss=0.5511642764087835\n",
      "Current iteration=6100, loss=0.5511338765435165\n",
      "Current iteration=6200, loss=0.551103604189262\n",
      "Current iteration=6300, loss=0.5510734585625991\n",
      "Current iteration=6400, loss=0.5510434388890302\n",
      "Current iteration=6500, loss=0.5510135444024079\n",
      "Current iteration=6600, loss=0.5509837743444705\n",
      "Current iteration=6700, loss=0.5509541279644612\n",
      "Current iteration=6800, loss=0.5509246045188129\n",
      "Current iteration=6900, loss=0.5508952032708848\n",
      "Current iteration=7000, loss=0.5508659234907411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=7100, loss=0.5508367644549621\n",
      "Current iteration=7200, loss=0.550807725446479\n",
      "Current iteration=7300, loss=0.5507788057544324\n",
      "Current iteration=7400, loss=0.5507500046740433\n",
      "Current iteration=7500, loss=0.5507213215065004\n",
      "Current iteration=7600, loss=0.5506927555588561\n",
      "Current iteration=7700, loss=0.5506643061439299\n",
      "Current iteration=7800, loss=0.5506359725802211\n",
      "Current iteration=7900, loss=0.5506077541918251\n",
      "Current iteration=8000, loss=0.5505796503083553\n",
      "Current iteration=8100, loss=0.5505516602648672\n",
      "Current iteration=8200, loss=0.5505237834017884\n",
      "Current iteration=8300, loss=0.5504960190648486\n",
      "Current iteration=8400, loss=0.5504683666050135\n",
      "Current iteration=8500, loss=0.5504408253784199\n",
      "Current iteration=8600, loss=0.5504133947463137\n",
      "Current iteration=8700, loss=0.5503860740749871\n",
      "Current iteration=8800, loss=0.5503588627357212\n",
      "Current iteration=8900, loss=0.5503317601047258\n",
      "Current iteration=9000, loss=0.5503047655630822\n",
      "Current iteration=9100, loss=0.5502778784966872\n",
      "Current iteration=9200, loss=0.5502510982961994\n",
      "Current iteration=9300, loss=0.5502244243569829\n",
      "Current iteration=9400, loss=0.5501978560790552\n",
      "Current iteration=9500, loss=0.5501713928670356\n",
      "Current iteration=9600, loss=0.5501450341300927\n",
      "Current iteration=9700, loss=0.5501187792818941\n",
      "Current iteration=9800, loss=0.5500926277405576\n",
      "Current iteration=9900, loss=0.5500665789286007\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5482646584792186\n",
      "Current iteration=200, loss=0.5476340499915696\n",
      "Current iteration=300, loss=0.5421395605555164\n",
      "Current iteration=400, loss=0.5350430282871954\n",
      "Current iteration=500, loss=0.5310555161255603\n",
      "Current iteration=600, loss=0.5284745799776389\n",
      "Current iteration=700, loss=0.5267041111061318\n",
      "Current iteration=800, loss=0.5255005922464416\n",
      "Current iteration=900, loss=0.5246971678048947\n",
      "Current iteration=1000, loss=0.5241234729886168\n",
      "Current iteration=1100, loss=0.5236676929911434\n",
      "Current iteration=1200, loss=0.5232872160784598\n",
      "Current iteration=1300, loss=0.5229652073966592\n",
      "Current iteration=1400, loss=0.5226908765851275\n",
      "Current iteration=1500, loss=0.5224558839064533\n",
      "Current iteration=1600, loss=0.5222535925376749\n",
      "Current iteration=1700, loss=0.5220786637375789\n",
      "Current iteration=1800, loss=0.5219267643695156\n",
      "Current iteration=1900, loss=0.521794348965351\n",
      "Current iteration=2000, loss=0.5216784952597062\n",
      "Current iteration=2100, loss=0.5215767786563051\n",
      "Current iteration=2200, loss=0.5214871753846362\n",
      "Current iteration=2300, loss=0.5214079870412007\n",
      "Current iteration=2400, loss=0.5213377812427897\n",
      "Current iteration=2500, loss=0.5212753445439732\n",
      "Current iteration=2600, loss=0.5212196447807215\n",
      "Current iteration=2700, loss=0.5211698007254413\n",
      "Current iteration=2800, loss=0.521125057462312\n",
      "Current iteration=2900, loss=0.521084766274614\n",
      "Current iteration=3000, loss=0.5210483681182826\n",
      "Current iteration=3100, loss=0.5210153799664097\n",
      "Current iteration=3200, loss=0.520985383467593\n",
      "Current iteration=3300, loss=0.5209580154809311\n",
      "Current iteration=3400, loss=0.5209329601420731\n",
      "Current iteration=3500, loss=0.5209099421852772\n",
      "Current iteration=3600, loss=0.5208887213011745\n",
      "Current iteration=3700, loss=0.5208690873526912\n",
      "Current iteration=3800, loss=0.5208508563052309\n",
      "Current iteration=3900, loss=0.5208338667538486\n",
      "Current iteration=4000, loss=0.5208179769513722\n",
      "Current iteration=4100, loss=0.520803062258424\n",
      "Current iteration=4200, loss=0.520789012949994\n",
      "Current iteration=4300, loss=0.5207757323243131\n",
      "Current iteration=4400, loss=0.5207631350688016\n",
      "Current iteration=4500, loss=0.5207511458452587\n",
      "Current iteration=4600, loss=0.5207396980625212\n",
      "Current iteration=4700, loss=0.5207287328098289\n",
      "Current iteration=4800, loss=0.5207181979282783\n",
      "Current iteration=4900, loss=0.5207080472011927\n",
      "Current iteration=5000, loss=0.5206982396471116\n",
      "Current iteration=5100, loss=0.5206887389015167\n",
      "Current iteration=5200, loss=0.5206795126754235\n",
      "Current iteration=5300, loss=0.5206705322806835\n",
      "Current iteration=5400, loss=0.5206617722132711\n",
      "Current iteration=5500, loss=0.5206532097870588\n",
      "Current iteration=5600, loss=0.5206448248116093\n",
      "Current iteration=5700, loss=0.5206365993084112\n",
      "Current iteration=5800, loss=0.5206285172607203\n",
      "Current iteration=5900, loss=0.5206205643928259\n",
      "Current iteration=6000, loss=0.5206127279751117\n",
      "Current iteration=6100, loss=0.5206049966517492\n",
      "Current iteration=6200, loss=0.5205973602882776\n",
      "Current iteration=6300, loss=0.5205898098366727\n",
      "Current iteration=6400, loss=0.5205823372158105\n",
      "Current iteration=6500, loss=0.5205749352055004\n",
      "Current iteration=6600, loss=0.5205675973524814\n",
      "Current iteration=6700, loss=0.5205603178869823\n",
      "Current iteration=6800, loss=0.520553091648615\n",
      "Current iteration=6900, loss=0.5205459140205175\n",
      "Current iteration=7000, loss=0.5205387808707983\n",
      "Current iteration=7100, loss=0.5205316885004472\n",
      "Current iteration=7200, loss=0.5205246335969724\n",
      "Current iteration=7300, loss=0.5205176131931183\n",
      "Current iteration=7400, loss=0.52051062463009\n",
      "Current iteration=7500, loss=0.5205036655247773\n",
      "Current iteration=7600, loss=0.5204967337405326\n",
      "Current iteration=7700, loss=0.5204898273611078\n",
      "Current iteration=7800, loss=0.5204829446673992\n",
      "Current iteration=7900, loss=0.5204760841166889\n",
      "Current iteration=8000, loss=0.5204692443241131\n",
      "Current iteration=8100, loss=0.5204624240461087\n",
      "Current iteration=8200, loss=0.5204556221656232\n",
      "Current iteration=8300, loss=0.5204488376788988\n",
      "Current iteration=8400, loss=0.5204420696836566\n",
      "Current iteration=8500, loss=0.5204353173685313\n",
      "Current iteration=8600, loss=0.5204285800036227\n",
      "Current iteration=8700, loss=0.5204218569320406\n",
      "Current iteration=8800, loss=0.5204151475623405\n",
      "Current iteration=8900, loss=0.5204084513617525\n",
      "Current iteration=9000, loss=0.520401767850119\n",
      "Current iteration=9100, loss=0.5203950965944677\n",
      "Current iteration=9200, loss=0.5203884372041498\n",
      "Current iteration=9300, loss=0.5203817893264862\n",
      "Current iteration=9400, loss=0.5203751526428646\n",
      "Current iteration=9500, loss=0.520368526865245\n",
      "Current iteration=9600, loss=0.520361911733026\n",
      "Current iteration=9700, loss=0.5203553070102352\n",
      "Current iteration=9800, loss=0.520348712483013\n",
      "Current iteration=9900, loss=0.5203421279573527\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5630818677920413\n",
      "Current iteration=200, loss=0.5482363307526217\n",
      "Current iteration=300, loss=0.5436386345347468\n",
      "Current iteration=400, loss=0.5413586652371807\n",
      "Current iteration=500, loss=0.5406918339089748\n",
      "Current iteration=600, loss=0.5392001288708664\n",
      "Current iteration=700, loss=0.5377107584764775\n",
      "Current iteration=800, loss=0.536736952034524\n",
      "Current iteration=900, loss=0.5360692676041018\n",
      "Current iteration=1000, loss=0.535552432620094\n",
      "Current iteration=1100, loss=0.5351222086266417\n",
      "Current iteration=1200, loss=0.5347541660837616\n",
      "Current iteration=1300, loss=0.5344356818704034\n",
      "Current iteration=1400, loss=0.534158010433506\n",
      "Current iteration=1500, loss=0.5339144271549908\n",
      "Current iteration=1600, loss=0.5336995958566948\n",
      "Current iteration=1700, loss=0.533509213836659\n",
      "Current iteration=1800, loss=0.5333397699208364\n",
      "Current iteration=1900, loss=0.5331883700464576\n",
      "Current iteration=2000, loss=0.5330526080413213\n",
      "Current iteration=2100, loss=0.5329304679665657\n",
      "Current iteration=2200, loss=0.5328202491285904\n",
      "Current iteration=2300, loss=0.5327205077354411\n",
      "Current iteration=2400, loss=0.5326300109951088\n",
      "Current iteration=2500, loss=0.5325477006525364\n",
      "Current iteration=2600, loss=0.5324726637761231\n",
      "Current iteration=2700, loss=0.5324041091714565\n",
      "Current iteration=2800, loss=0.532341348203557\n",
      "Current iteration=2900, loss=0.5322837791014065\n",
      "Current iteration=3000, loss=0.5322308740337248\n",
      "Current iteration=3100, loss=0.53218216840534\n",
      "Current iteration=3200, loss=0.5321372519443142\n",
      "Current iteration=3300, loss=0.5320957612418725\n",
      "Current iteration=3400, loss=0.5320573734776681\n",
      "Current iteration=3500, loss=0.532021801117371\n",
      "Current iteration=3600, loss=0.5319887874119541\n",
      "Current iteration=3700, loss=0.5319581025612447\n",
      "Current iteration=3800, loss=0.5319295404304525\n",
      "Current iteration=3900, loss=0.5319029157291167\n",
      "Current iteration=4000, loss=0.5318780615784109\n",
      "Current iteration=4100, loss=0.5318548274059693\n",
      "Current iteration=4200, loss=0.5318330771180295\n",
      "Current iteration=4300, loss=0.5318126875072795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4400, loss=0.5317935468617921\n",
      "Current iteration=4500, loss=0.5317755537461075\n",
      "Current iteration=4600, loss=0.5317586159302042\n",
      "Current iteration=4700, loss=0.5317426494459284\n",
      "Current iteration=4800, loss=0.5317275777536195\n",
      "Current iteration=4900, loss=0.5317133310042965\n",
      "Current iteration=5000, loss=0.5316998453849591\n",
      "Current iteration=5100, loss=0.5316870625363694\n",
      "Current iteration=5200, loss=0.5316749290342218\n",
      "Current iteration=5300, loss=0.5316633959258841\n",
      "Current iteration=5400, loss=0.5316524183159805\n",
      "Current iteration=5500, loss=0.5316419549950016\n",
      "Current iteration=5600, loss=0.5316319681059061\n",
      "Current iteration=5700, loss=0.5316224228443428\n",
      "Current iteration=5800, loss=0.5316132871886805\n",
      "Current iteration=5900, loss=0.5316045316565273\n",
      "Current iteration=6000, loss=0.5315961290848271\n",
      "Current iteration=6100, loss=0.5315880544309857\n",
      "Current iteration=6200, loss=0.5315802845927832\n",
      "Current iteration=6300, loss=0.5315727982450995\n",
      "Current iteration=6400, loss=0.5315655756917103\n",
      "Current iteration=6500, loss=0.531558598730612\n",
      "Current iteration=6600, loss=0.5315518505315087\n",
      "Current iteration=6700, loss=0.531545315524254\n",
      "Current iteration=6800, loss=0.5315389792971623\n",
      "Current iteration=6900, loss=0.5315328285042358\n",
      "Current iteration=7000, loss=0.5315268507804453\n",
      "Current iteration=7100, loss=0.5315210346643037\n",
      "Current iteration=7200, loss=0.5315153695270424\n",
      "Current iteration=7300, loss=0.5315098455077778\n",
      "Current iteration=7400, loss=0.5315044534541183\n",
      "Current iteration=7500, loss=0.5314991848677116\n",
      "Current iteration=7600, loss=0.5314940318542912\n",
      "Current iteration=7700, loss=0.5314889870778144\n",
      "Current iteration=7800, loss=0.5314840437183348\n",
      "Current iteration=7900, loss=0.5314791954332758\n",
      "Current iteration=8000, loss=0.5314744363218132\n",
      "Current iteration=8100, loss=0.5314697608920963\n",
      "Current iteration=8200, loss=0.5314651640310627\n",
      "Current iteration=8300, loss=0.531460640976632\n",
      "Current iteration=8400, loss=0.5314561872920703\n",
      "Current iteration=8500, loss=0.531451798842349\n",
      "Current iteration=8600, loss=0.5314474717723313\n",
      "Current iteration=8700, loss=0.5314432024866322\n",
      "Current iteration=8800, loss=0.53143898763102\n",
      "Current iteration=8900, loss=0.5314348240752298\n",
      "Current iteration=9000, loss=0.5314307088970756\n",
      "Current iteration=9100, loss=0.5314266393677575\n",
      "Current iteration=9200, loss=0.5314226129382685\n",
      "Current iteration=9300, loss=0.5314186272268114\n",
      "Current iteration=9400, loss=0.5314146800071494\n",
      "Current iteration=9500, loss=0.5314107691978142\n",
      "Current iteration=9600, loss=0.5314068928521053\n",
      "Current iteration=9700, loss=0.5314030491488211\n",
      "Current iteration=9800, loss=0.5313992363836625\n",
      "Current iteration=9900, loss=0.5313954529612576\n",
      "(227458,)\n",
      "227458\n",
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss0, w0 = implementations.reg_logistic_regression(y0_bis, tx0_std, 0.0000009, np.ones(len(tx0[0])), 10000, 0.852)\n",
    "loss1, w1 = implementations.reg_logistic_regression(y1_bis, tx1_std, 0.0000009, np.ones(len(tx1[0])), 10000, 0.532)\n",
    "loss2, w2 = implementations.reg_logistic_regression(y2_bis, tx2_std, 0.0000009, np.ones(len(tx2[0])), 10000, 0.701)\n",
    "loss3, w3 = implementations.reg_logistic_regression(y3_bis, tx3_std, 0.0000009, np.ones(len(tx3[0])), 10000, 0.456)\n",
    "\n",
    "\"\"\"\n",
    "loss0, w0 = implementations.mean_squared_error_gd(y0_bis, tx0, np.ones(len(tx0[0])), 10000, 0.05)\n",
    "loss1, w1 = implementations.mean_squared_error_gd(y1_bis, tx1, np.ones(len(tx1[0])), 10000, 0.05)\n",
    "loss2, w2 = implementations.mean_squared_error_gd(y2_bis, tx2, np.ones(len(tx2[0])), 10000, 0.05)\n",
    "loss3, w3 = implementations.mean_squared_error_gd(y3_bis, tx3, np.ones(len(tx3[0])), 10000, 0.05)\n",
    "\"\"\"\n",
    "\n",
    "r0 = implementations.sigmoid(t0_std @ w0)\n",
    "r0_pred = np.sign(r0 - 0.5)\n",
    "r1 = implementations.sigmoid(t1_std @ w1)\n",
    "r1_pred = np.sign(r1 - 0.5)\n",
    "r2 = implementations.sigmoid(t2_std @ w2)\n",
    "r2_pred = np.sign(r2 - 0.5)\n",
    "r3 = implementations.sigmoid(t3_std @ w3)\n",
    "r3_pred = np.sign(r3 - 0.5)\n",
    "\n",
    "pred4V = create_with_index(r0_pred, r1_pred, r2_pred, r3_pred, idx0, idx1, idx2, idx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a6b1f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5635411294217453\n",
      "Current iteration=200, loss=0.5484173084207198\n",
      "Current iteration=300, loss=0.5437642081372596\n",
      "Current iteration=400, loss=0.541237003865525\n",
      "Current iteration=500, loss=0.539583519209969\n",
      "Current iteration=600, loss=0.5383872264568852\n",
      "Current iteration=700, loss=0.5374663312215433\n",
      "Current iteration=800, loss=0.5367286023596053\n",
      "Current iteration=900, loss=0.5361216417832608\n",
      "iter 0 : 0.5356175407258165\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5634635159114842\n",
      "Current iteration=200, loss=0.5483868110828837\n",
      "Current iteration=300, loss=0.5437425539414262\n",
      "Current iteration=400, loss=0.5412193071855742\n",
      "Current iteration=500, loss=0.5395681370555353\n",
      "Current iteration=600, loss=0.5383733727325922\n",
      "Current iteration=700, loss=0.5374535978933613\n",
      "Current iteration=800, loss=0.5367167660513131\n",
      "Current iteration=900, loss=0.536110571553749\n",
      "iter 1 : 0.5356071459440265\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.563386334563052\n",
      "Current iteration=200, loss=0.5483564487718536\n",
      "Current iteration=300, loss=0.5437209837704522\n",
      "Current iteration=400, loss=0.5412016764965134\n",
      "Current iteration=500, loss=0.5395528095299478\n",
      "Current iteration=600, loss=0.538359566748517\n",
      "Current iteration=700, loss=0.5374409080814896\n",
      "Current iteration=800, loss=0.5367049705049656\n",
      "Current iteration=900, loss=0.536099540167805\n",
      "iter 2 : 0.5355967885598008\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5633095822075186\n",
      "Current iteration=200, loss=0.5483262204541592\n",
      "Current iteration=300, loss=0.5436995004054429\n",
      "Current iteration=400, loss=0.5411841343082252\n",
      "Current iteration=500, loss=0.5395375518495719\n",
      "Current iteration=600, loss=0.5383458103745906\n",
      "Current iteration=700, loss=0.5374282616221235\n",
      "Current iteration=800, loss=0.5366932154887172\n",
      "Current iteration=900, loss=0.5360885474123016\n",
      "iter 3 : 0.5355864683724934\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5632332557015776\n",
      "Current iteration=200, loss=0.5482961251561694\n",
      "Current iteration=300, loss=0.5436781361427477\n",
      "Current iteration=400, loss=0.5411672268078569\n",
      "Current iteration=500, loss=0.5395232529762645\n",
      "Current iteration=600, loss=0.5383324022669317\n",
      "Current iteration=700, loss=0.5374156921765657\n",
      "Current iteration=800, loss=0.5366815020713551\n",
      "Current iteration=900, loss=0.5360775927276438\n",
      "iter 4 : 0.5355761848961019\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5631573519273256\n",
      "Current iteration=200, loss=0.5482661620674358\n",
      "Current iteration=300, loss=0.5436571717180207\n",
      "Current iteration=400, loss=0.5411622267037125\n",
      "Current iteration=500, loss=0.5395528528667449\n",
      "Current iteration=600, loss=0.5383527961215021\n",
      "Current iteration=700, loss=0.5374123319675453\n",
      "Current iteration=800, loss=0.5366710284836265\n",
      "Current iteration=900, loss=0.5360667501516118\n",
      "iter 5 : 0.5355659252067974\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5630818677920413\n",
      "Current iteration=200, loss=0.5482363307526217\n",
      "Current iteration=300, loss=0.5436386345347468\n",
      "Current iteration=400, loss=0.5413586652371807\n",
      "Current iteration=500, loss=0.5406918339089748\n",
      "Current iteration=600, loss=0.5392001288708664\n",
      "Current iteration=700, loss=0.5377107584764775\n",
      "Current iteration=800, loss=0.536736952034524\n",
      "Current iteration=900, loss=0.5360692676041018\n",
      "iter 6 : 0.5355571154736246\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5630068002279678\n",
      "Current iteration=200, loss=0.5482066312189897\n",
      "Current iteration=300, loss=0.543633888432363\n",
      "Current iteration=400, loss=0.5434909433197186\n",
      "Current iteration=500, loss=0.542865368597514\n",
      "Current iteration=600, loss=0.5398376996223402\n",
      "Current iteration=700, loss=0.5380706036062635\n",
      "Current iteration=800, loss=0.5369284608867712\n",
      "Current iteration=900, loss=0.5361417542565655\n",
      "iter 7 : 0.5355728524178384\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5629321461920974\n",
      "Current iteration=200, loss=0.5481770622273674\n",
      "Current iteration=300, loss=0.5436809475750828\n",
      "Current iteration=400, loss=0.5480268622446525\n",
      "Current iteration=500, loss=0.5433123503977503\n",
      "Current iteration=600, loss=0.5404022054766638\n",
      "Current iteration=700, loss=0.5386273716967911\n",
      "Current iteration=800, loss=0.5373712060700243\n",
      "Current iteration=900, loss=0.5364424756103366\n",
      "iter 8 : 0.5357508131518337\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.562857902665962\n",
      "Current iteration=200, loss=0.5481476149956824\n",
      "Current iteration=300, loss=0.5437459457892007\n",
      "Current iteration=400, loss=0.549807707016448\n",
      "Current iteration=500, loss=0.543953480856142\n",
      "Current iteration=600, loss=0.541164328728508\n",
      "Current iteration=700, loss=0.5393684811745308\n",
      "Current iteration=800, loss=0.5380510171710104\n",
      "Current iteration=900, loss=0.5370436593991259\n",
      "iter 9 : 0.5362792043943353\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=100, loss=0.5627840666554293\n",
      "Current iteration=200, loss=0.5481182908103485\n",
      "Current iteration=300, loss=0.5437100993658256\n",
      "Current iteration=400, loss=0.552105368574981\n",
      "Current iteration=500, loss=0.5452040906327704\n",
      "Current iteration=600, loss=0.5422591150357291\n",
      "Current iteration=700, loss=0.540362927973007\n",
      "Current iteration=800, loss=0.5389737607792287\n",
      "Current iteration=900, loss=0.5379134662694571\n",
      "iter 10 : 0.5370247192595422\n",
      "Current iteration=0, loss=inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_109715/4033712945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.45\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplementations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my3_bis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx3_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0000009\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iter \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/ML_2022/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/ML_2022/implementations.py\u001b[0m in \u001b[0;36mc_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0;34m\"\"\"Logistic loss\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     loss = -(y.T.dot(np.log(predict)) + (1.0 - y).T.dot(np.log(1.0 - predict))) / len(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    gamma = 0.001 * i\n",
    "    gamma += 0.456\n",
    "    loss0, w0 = implementations.reg_logistic_regression(y3_bis, tx3_std, 0.0000009, np.ones(len(tx3[0])), 1000, gamma)\n",
    "    print(\"iter \" + str(i) + \" : \" + str(loss0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b6016c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples = len(test_data[0])\n",
    "#txTE = np.c_[np.ones(num_samples), test_data[1]]\n",
    "\n",
    "\n",
    "#rig_log\n",
    "#print(txTE.shape, w1.shape)\n",
    "#prediTE = implementations.sigmoid(txTE @ opt_wTR)\n",
    "#predi_tTE = np.sign(prediTE - 0.5)\n",
    "#print(predi_tTE.shape)\n",
    "\n",
    "ids_ = test_data[2]\n",
    "y_pred = pred4V\n",
    "name = \"logistic_ridge_7.csv\"\n",
    "create_csv_submission(ids_, y_pred, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a741ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164333\n",
      "85667\n"
     ]
    }
   ],
   "source": [
    "x_test = train_data[0]\n",
    "print(len(x_test[x_test == -1]))\n",
    "print(len(x_test[x_test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 0\n",
    "\n",
    "for i in range(len(pred4V)):\n",
    "        if(pred4V[i] != np.array(test_data[0])[i]):\n",
    "            test += 1 \n",
    "        \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross testing + lambda testing\n",
    "\"\"\" Remarks for report: to find the best lambda, we manually tested them by narrowing our range.\n",
    "    First, 1-10 (1), 0.1-1 (1), 1-2 (ev. faire du '0.1-2')\n",
    "\"\"\"\n",
    "#form K subgroups randomly\n",
    "k_fold = 10 #to change when on better computer\n",
    "seed = 0\n",
    "k_indices = build_k_indices(y0, k_fold, seed)\n",
    "#take a range of lambda\n",
    "#lambdas = np.logspace(-5, 0, 15)\n",
    "lambdas = np.linspace(0.1,1,10)\n",
    "print(lambdas)\n",
    "#initialize the variables\n",
    "rmse_tr_tmp = []\n",
    "rmse_te_tmp = []\n",
    "w_tr_tmp = []\n",
    "w_te_tmp = []\n",
    "opt_rmse_te = np.Inf\n",
    "opt_rmse_tr = np.Inf\n",
    "opt_lambda = 0\n",
    "opt_wTR = np.ones(len(y))\n",
    "for lambda_ in lambdas:\n",
    "    for k in range(k_fold):\n",
    "        # Put the kth group in test\n",
    "        te_indice = k_indices[k]\n",
    "        tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "        tr_indice = tr_indice.reshape(-1)\n",
    "        #create new x and y for test and training set\n",
    "        y_te = y[te_indice]\n",
    "        y_tr = y[tr_indice]\n",
    "        x_te = tx0[te_indice]\n",
    "        x_tr = tx0[tr_indice]\n",
    "        print(lambda_)\n",
    "        loss_tr, w_tr = implementations.logistic_regression(y_tr, x_tr, np.ones(len(x_tr[0])), 10000, lambda_)\n",
    "        loss_te, w_te = implementations.logistic_regression(y_te, x_te, np.ones(len(x_te[0])), 10000, lambda_)\n",
    "        rmse_tr_tmp.append(loss_tr)\n",
    "        rmse_te_tmp.append(loss_te)\n",
    "        w_tr_tmp.append(w_tr)\n",
    "        w_te_tmp.append(w_te)\n",
    "    rmse_tr = np.mean(rmse_tr_tmp, axis=0)\n",
    "    rmse_te = np.mean(rmse_te_tmp, axis=0)\n",
    "    wTR = np.mean(w_tr_tmp, axis=0)\n",
    "    wTE = np.mean(w_te_tmp, axis=0)\n",
    "    if rmse_te < opt_rmse_te:\n",
    "        opt_rmse_te = rmse_te\n",
    "        opt_rmse_tr = rmse_tr\n",
    "        opt_lambda = lambda_\n",
    "        opt_wTR = wTR\n",
    "\n",
    "\n",
    "#return opt_rms_tr, opt_lambda, opt_wTR\n",
    "print(opt_rmse_tr, opt_lambda, opt_wTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cf1c4-7a05-4403-9550-45bf7c192e47",
   "metadata": {},
   "source": [
    "## Visualization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef94df50-c58e-4d54-b84c-3f89cb6bc128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the correlation values betweeny and each feature against the ids of the features\n",
    "def corrplot(ind, corr):\n",
    "    plt.scatter(ind, corr, )\n",
    "    plt.axhline(0, c='r', ls=':')\n",
    "    plt.title(\"Correlations coefficient of the different features of x\")\n",
    "    plt.ylabel(\"Correlation coefficients\")\n",
    "    plt.xlabel(\"Features' indices\")\n",
    "    plt.savefig('Corrplot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "#compute the correlation coeffincients between y and each of the features of x\n",
    "corr0 = []\n",
    "corr1 = []\n",
    "corr2 = []\n",
    "corr3 = []\n",
    "for i in range(len(tx0[1])):\n",
    "    feat0 = extract(tx0,i)\n",
    "    corr0.append(np.corrcoef(y0,feat0)[0,1])\n",
    "for i in range(len(tx1[1])):\n",
    "    feat1 = extract(tx1,i)\n",
    "    corr1.append(np.corrcoef(y1,feat1)[0,1])\n",
    "for i in range(len(tx2[1])):\n",
    "    feat2 = extract(tx2,i)\n",
    "    corr2.append(np.corrcoef(y2,feat2)[0,1])\n",
    "for i in range(len(tx3[1])):\n",
    "    feat3 = extract(tx3,i)\n",
    "    corr3.append(np.corrcoef(y3,feat3)[0,1])\n",
    "\n",
    "#get an array corresponding to the indices of each feature\n",
    "def listFrom1toN(n):\n",
    "    return list(range(0,n))\n",
    "ind0 = listFrom1toN(len(tx0[1]))\n",
    "ind1 = listFrom1toN(len(tx1[1]))\n",
    "ind2 = listFrom1toN(len(tx2[1]))\n",
    "ind3 = listFrom1toN(len(tx3[1]))\n",
    "#print(ind)\n",
    "\n",
    "corrplot(ind0, corr0)\n",
    "#corrplot(ind1, corr1)\n",
    "print(ind1)\n",
    "print(ind2)\n",
    "print(ind3)\n",
    "#corrplot(ind2, corr2)\n",
    "#corrplot(ind3, corr3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dafc7a-7dfe-4d4c-8b79-a2e37a5ea06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d'apres le graph 0: 2,5,7,8,9,10,14,15\n",
    "#d'apres le graph 1: 2,4,7,8,9,10,15,16,18\n",
    "#d'apres le graph 2: 2,4,5,6,7,9,10,11,12,13,14,21,23,25\n",
    "#d'apres le graph 3: 2,4,5,6,7,11,12,13,14,18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
